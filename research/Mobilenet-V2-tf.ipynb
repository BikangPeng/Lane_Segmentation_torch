{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:14.416846Z",
     "start_time": "2020-04-09T14:20:12.998102Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:10:37.787561Z",
     "start_time": "2020-04-09T14:10:36.627593Z"
    }
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train, x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    mean = [125.307, 122.95, 113.865]\n",
    "    std = [62.9932, 62.0887, 66.7048]\n",
    "    for i in range(3):\n",
    "        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]\n",
    "        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]\n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mobilnet-v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:16.324599Z",
     "start_time": "2020-04-09T14:20:16.296795Z"
    }
   },
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 in_planes,\n",
    "                 out_planes,\n",
    "                 expansion,\n",
    "                 stride,\n",
    "                 weight_decay=1e-4):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        planes = in_planes * expansion\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=planes,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='valid',\n",
    "            use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay))\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.DepthwiseConv2D(\n",
    "            kernel_size=3,\n",
    "            strides=stride,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "            depthwise_initializer='he_normal',\n",
    "            depthwise_regularizer=keras.regularizers.l2(weight_decay))\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(\n",
    "            filters=out_planes,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='valid',\n",
    "            use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay))\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = keras.Sequential()\n",
    "            self.shortcut.add(\n",
    "                layers.Conv2D(\n",
    "                    filters=out_planes,\n",
    "                    kernel_size=1,\n",
    "                    strides=1,\n",
    "                    padding='valid',\n",
    "                    use_bias=False,\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "            self.shortcut.add(layers.BatchNormalization())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.bn1(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.bn2(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        outputs = self.bn3(outputs)\n",
    "        if self.stride == 1:\n",
    "            if self.in_planes == self.out_planes:\n",
    "                outputs += inputs\n",
    "            else:\n",
    "                outputs += self.shortcut(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:17.429976Z",
     "start_time": "2020-04-09T14:20:16.915883Z"
    }
   },
   "outputs": [],
   "source": [
    "class MobileNet(keras.Model):\n",
    "    # (expansion, out_planes, num_blocks, stride)\n",
    "    cfg = [\n",
    "        (1, 16, 1, 1),\n",
    "        (6, 24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n",
    "        (6, 32, 3, 2),\n",
    "        (6, 64, 4, 2),\n",
    "        (6, 96, 3, 1),\n",
    "        (6, 160, 3, 2),\n",
    "        (6, 320, 1, 1)\n",
    "    ]\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_decay=1e-4):\n",
    "        super(MobileNet, self).__init__()\n",
    "        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay))\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.sequential = self._make_layers(in_planes=32)\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            filters=1280,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay))\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.pooling = layers.GlobalAveragePooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=keras.regularizers.l2(weight_decay))\n",
    "    \n",
    "    def _make_layers(self, in_planes):\n",
    "        sequential = keras.Sequential()\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1] * (num_blocks - 1) # 下采样只在每个bottlenect的第一次\n",
    "            for stride in strides:\n",
    "                sequential.add(Block(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return sequential\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.bn1(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.sequential(outputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.bn2(outputs)\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        outputs = self.pooling(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.dense(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T08:26:13.631931Z",
     "start_time": "2020-04-09T08:26:13.608291Z"
    }
   },
   "source": [
    "## 构建Mobilenet 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:18.368664Z",
     "start_time": "2020-04-09T14:20:17.510546Z"
    }
   },
   "outputs": [],
   "source": [
    "# def MobileNetV2(input_shape, num_classes=10, weight_decay=1e-4):\n",
    "#     model = MobileNet(num_classes=num_classes, weight_decay=weight_decay)\n",
    "#     model.build(input_shape=input_shape)\n",
    "#     return model\n",
    "\n",
    "def MobileNetV2(input_shape, num_classes=10, weight_decay=1e-4):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    outputs = MobileNet(num_classes=num_classes, weight_decay=weight_decay)(inputs)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:18.449556Z",
     "start_time": "2020-04-09T14:20:18.447502Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# model = MobileNetV2(input_shape=(None, 224, 224, 3))\n",
    "# model = MobileNetV2(input_shape=(224, 224, 3))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理 parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:18.773653Z",
     "start_time": "2020-04-09T14:20:18.746964Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:18.932404Z",
     "start_time": "2020-04-09T14:20:18.928812Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        dict = pickle.load(f, encoding='bytes')\n",
    "        x = dict[b'data']\n",
    "        y = dict[b'labels']\n",
    "        x = np.reshape(x, (10000, 3, 32, 32))\n",
    "        x = np.transpose(x, (0, 2, 3, 1))\n",
    "        y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:19.092380Z",
     "start_time": "2020-04-09T14:20:19.087979Z"
    }
   },
   "outputs": [],
   "source": [
    "def save(dir, x, y):\n",
    "    assert x.shape[0] == y.shape[0], 'x num is not equal with y'\n",
    "    for i, label in enumerate(y):\n",
    "        sub_dir = os.path.join(dir, str(label))\n",
    "        if os.path.exists(sub_dir):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(sub_dir)\n",
    "        path = os.path.join(sub_dir, str(i) + '.png')\n",
    "        image = cv2.cvtColor(x[i], cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(path, image)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:19.254638Z",
     "start_time": "2020-04-09T14:20:19.251168Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse(input_dir, output_dir, file_names):\n",
    "    for name in file_names:\n",
    "        file = os.path.join(input_dir, name)\n",
    "        x, y = unpickle(file)\n",
    "        save(output_dir, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:10:54.166363Z",
     "start_time": "2020-04-09T14:10:40.588975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n",
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n",
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n",
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n",
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "train = [\n",
    "    'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4',\n",
    "    'data_batch_5'\n",
    "]\n",
    "test = ['test_batch']\n",
    "input_dir = '/home/kang/CV/DATASETS/cifar10/original'\n",
    "train_dir = '/home/kang/CV/DATASETS/cifar10/train'\n",
    "test_dir = '/home/kang/CV/DATASETS/cifar10/test'\n",
    "\n",
    "parse(input_dir, train_dir, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:10:56.556050Z",
     "start_time": "2020-04-09T14:10:54.257096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n",
      "599\n",
      "699\n",
      "799\n",
      "899\n",
      "999\n",
      "1099\n",
      "1199\n",
      "1299\n",
      "1399\n",
      "1499\n",
      "1599\n",
      "1699\n",
      "1799\n",
      "1899\n",
      "1999\n",
      "2099\n",
      "2199\n",
      "2299\n",
      "2399\n",
      "2499\n",
      "2599\n",
      "2699\n",
      "2799\n",
      "2899\n",
      "2999\n",
      "3099\n",
      "3199\n",
      "3299\n",
      "3399\n",
      "3499\n",
      "3599\n",
      "3699\n",
      "3799\n",
      "3899\n",
      "3999\n",
      "4099\n",
      "4199\n",
      "4299\n",
      "4399\n",
      "4499\n",
      "4599\n",
      "4699\n",
      "4799\n",
      "4899\n",
      "4999\n",
      "5099\n",
      "5199\n",
      "5299\n",
      "5399\n",
      "5499\n",
      "5599\n",
      "5699\n",
      "5799\n",
      "5899\n",
      "5999\n",
      "6099\n",
      "6199\n",
      "6299\n",
      "6399\n",
      "6499\n",
      "6599\n",
      "6699\n",
      "6799\n",
      "6899\n",
      "6999\n",
      "7099\n",
      "7199\n",
      "7299\n",
      "7399\n",
      "7499\n",
      "7599\n",
      "7699\n",
      "7799\n",
      "7899\n",
      "7999\n",
      "8099\n",
      "8199\n",
      "8299\n",
      "8399\n",
      "8499\n",
      "8599\n",
      "8699\n",
      "8799\n",
      "8899\n",
      "8999\n",
      "9099\n",
      "9199\n",
      "9299\n",
      "9399\n",
      "9499\n",
      "9599\n",
      "9699\n",
      "9799\n",
      "9899\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "parse(input_dir, test_dir, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据生成 generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:25.717717Z",
     "start_time": "2020-04-09T14:20:25.715117Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义生成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:26.147530Z",
     "start_time": "2020-04-09T14:20:26.119766Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, num_classes, is_shuffle, is_horizontal_flip,\n",
    "                 is_random_crop, is_random_cutout):\n",
    "        self.num_classes = num_classes\n",
    "        self.is_shuffle = is_shuffle\n",
    "        self.is_horizontal_flip = is_horizontal_flip\n",
    "        self.is_random_crop = is_random_crop\n",
    "        self.is_random_cutout = is_random_cutout\n",
    "\n",
    "    def random_horizontal_flip(self, image):\n",
    "        if random.random() <= 0.5:\n",
    "            return cv2.flip(image, 1)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def random_crop(self, image, padding=4):\n",
    "        if random.random() <= 0.5:\n",
    "            image = cv2.copyMakeBorder(image,\n",
    "                                       padding,\n",
    "                                       padding,\n",
    "                                       padding,\n",
    "                                       padding,\n",
    "                                       cv2.BORDER_CONSTANT,\n",
    "                                       value=[0, 0, 0])\n",
    "            x = np.random.randint(0, padding * 2)\n",
    "            y = np.random.randint(0, padding * 2)\n",
    "            image = image[y:y + 32, x:x + 32]\n",
    "            return image\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def random_cutout(self, image, offset=8):\n",
    "        if random.random() <= 0.5:\n",
    "            h, w, _ = image.shape\n",
    "            x = np.random.randint(0, w - offset)\n",
    "            y = np.random.randint(0, h - offset)\n",
    "            image[y:y + offset, x: x + offset, :] = 0\n",
    "            return image\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def preprocess(self, x, y):\n",
    "        x = x.astype('float32') / 255\n",
    "        x -= (0.4914, 0.4822, 0.4465)\n",
    "        x /= (0.2023, 0.1994, 0.2010)\n",
    "        return x, y\n",
    "    \n",
    "    def load(self, path):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    \n",
    "    def sample(self, image_path, label):\n",
    "        image = self.load(image_path)\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        one_hot = np.zeros(self.num_classes, dtype=np.int32)\n",
    "        one_hot[label] = 1\n",
    "        label = one_hot\n",
    "        image, label = self.preprocess(image, one_hot)\n",
    "        \n",
    "        if self.is_horizontal_flip:\n",
    "            image = self.random_horizontal_flip(image)\n",
    "        if self.is_random_crop:\n",
    "            image = self.random_crop(image)\n",
    "        if self.is_random_cutout:\n",
    "            image = self.random_cutout(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def generate(self, dataset):\n",
    "        if self.is_shuffle:\n",
    "            random.shuffle(dataset)\n",
    "        for image_path, label in dataset:\n",
    "            try:\n",
    "                data = self.sample(image_path, label)\n",
    "                yield data\n",
    "            except Exception as e:\n",
    "                traceback.print_tb(e.__traceback__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:26.505387Z",
     "start_time": "2020-04-09T14:20:26.495388Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(dir):\n",
    "    dataset = list()\n",
    "    for lable in os.listdir(dir):\n",
    "        sub_dir = os.path.join(dir, lable)\n",
    "        for name in os.listdir(sub_dir):\n",
    "            path = os.path.join(sub_dir, name)\n",
    "            dataset.append((path, int(lable)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:26.858799Z",
     "start_time": "2020-04-09T14:20:26.852874Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(dir,\n",
    "              num_classes,\n",
    "              is_shuffle=False,\n",
    "              is_horizontal_flip=False,\n",
    "              is_random_crop=False,\n",
    "              is_random_cutout=False):\n",
    "    dataset = get_data(dir)\n",
    "    gt = Generator(num_classes=num_classes,\n",
    "                   is_shuffle=is_shuffle,\n",
    "                   is_horizontal_flip=is_horizontal_flip,\n",
    "                   is_random_crop=is_random_crop,\n",
    "                   is_random_cutout=is_random_cutout)\n",
    "    return partial(gt.generate, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义学习率类 cb_learning_rate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:27.334631Z",
     "start_time": "2020-04-09T14:20:27.331685Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:27.542463Z",
     "start_time": "2020-04-09T14:20:27.533341Z"
    }
   },
   "outputs": [],
   "source": [
    "RANGE = 'range'\n",
    "EXPONENT = 'exponent'\n",
    "\n",
    "\n",
    "class LearningRate(object):\n",
    "    def __init__(self,\n",
    "                 optimizer=None,\n",
    "                 method=None,\n",
    "                 lr_range=None,\n",
    "                 initial_lr=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.method = method\n",
    "        self.lr_range = lr_range\n",
    "        self.initial_lr = initial_lr\n",
    "\n",
    "    def __call__(self, epoch, logs=None):\n",
    "        if self.optimizer is None:\n",
    "            raise ValueError('optimizer is none.')\n",
    "        if not hasattr(self.optimizer, 'learning_rate'):\n",
    "            raise ValueError(\n",
    "                'Optimizer must have a \"learning_rate\" attribute.')\n",
    "        # 从模型的优化器中获取当前的学习率。\n",
    "        lr = float(keras.backend.get_value(self.optimizer.learning_rate))\n",
    "\n",
    "        # 调用调度函数来获取预定的学习率。\n",
    "        if self.method == 'range':\n",
    "            scheduled_lr = self.adjust_range(epoch, lr)\n",
    "        elif self.method == 'exponent':\n",
    "            scheduled_lr = self.adjust_exponent(epoch)\n",
    "        else:\n",
    "            scheduled_lr = lr\n",
    "        # 在这个周期开始之前，将值设置为优化器的值。\n",
    "        keras.backend.set_value(self.optimizer.learning_rate, scheduled_lr)\n",
    "    \n",
    "    def adjust_range(self, epoch, lr):\n",
    "        if self.lr_range is None:\n",
    "            raise ValueError('lr_ranges is none.')\n",
    "        if epoch < self.lr_range[0][0] or epoch > self.lr_range[-1][0]:\n",
    "            return lr\n",
    "        for i in range(len(self.lr_range)-1, -1, -1):\n",
    "            if epoch >= self.lr_range[i][0]:\n",
    "                return self.lr_range[i][1]\n",
    "        return lr\n",
    "    \n",
    "    def adjust_exponent(self, epoch, lr):\n",
    "        if self.initial_lr is None:\n",
    "            raise ValueError('initial_lr is none.')\n",
    "        if epoch < 10:\n",
    "            return self.initial_lr\n",
    "        else:\n",
    "            return self.initial_lr * tf.math.exp(0.01 * (10 - epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数设置 config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:27.935764Z",
     "start_time": "2020-04-09T14:20:27.927424Z"
    }
   },
   "outputs": [],
   "source": [
    "width = 32\n",
    "height = 32\n",
    "num_channels = 3\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "epochs = 400\n",
    "show_every_steps = 100\n",
    "start_epoch = 0\n",
    "\n",
    "num_train_samples = 50000\n",
    "lr = 1e-1\n",
    "lr_range = [(150, 1e-2), (250, 1e-3), (350, 1e-4)]\n",
    "\n",
    "weight_decay = 5e-4\n",
    "label_smoothing = 0.2\n",
    "\n",
    "model_name = 'mobilenet_v2'\n",
    "resnet_version = 2\n",
    "resnet_depth = 164\n",
    "\n",
    "model_dir = './models/' + model_name + '/'\n",
    "summary_dir = './summaries/' + model_name + '/'\n",
    "\n",
    "train_dir = '/home/kang/CV/DATASETS/cifar10/train/'\n",
    "test_dir = '/home/kang/CV/DATASETS/cifar10/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:28.366016Z",
     "start_time": "2020-04-09T14:20:28.355462Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:28.716305Z",
     "start_time": "2020-04-09T14:20:28.694904Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model_dir,\n",
    "                 summary_dir,\n",
    "                 model,\n",
    "                 optimizer,\n",
    "                 lr_range=None,\n",
    "                 label_smoothing=0.2):\n",
    "        self.model_dir = model_dir\n",
    "        self.summary_dir = summary_dir\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_range = lr_range\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.cb_lr = self.load_cb_lr()\n",
    "        self.summary_writer = self.get_summary_writer()\n",
    "        self.create_model_dir()\n",
    "\n",
    "    def load_cb_lr(self):\n",
    "        return LearningRate(optimizer=self.optimizer,\n",
    "                            method=RANGE,\n",
    "                            lr_range=lr_range)\n",
    "\n",
    "    def create_model_dir(self):\n",
    "        if self.model_dir is None:\n",
    "            return None\n",
    "        self.model_dir = os.path.join(\n",
    "            self.model_dir,\n",
    "            datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "        os.makedirs(self.model_dir)\n",
    "\n",
    "    def get_summary_writer(self):\n",
    "        if self.summary_dir is None:\n",
    "            return None\n",
    "        else:\n",
    "            log_dir = os.path.join(\n",
    "                self.summary_dir,\n",
    "                datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "            return tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "    def update_summary(self, **kwargs):\n",
    "        if self.summary_writer is None:\n",
    "            pass\n",
    "        else:\n",
    "            with self.summary_writer.as_default():\n",
    "                for name in kwargs:\n",
    "                    tf.summary.scalar(name,\n",
    "                                      kwargs[name],\n",
    "                                      step=self.optimizer.iterations)\n",
    "\n",
    "    def save_weights(self, filepath, save_format=None):\n",
    "        self.model.save_weights(filepath=filepath, save_format=save_format)\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath=filepath)\n",
    "\n",
    "    @tf.function\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        res = keras.metrics.categorical_accuracy(y_true, y_pred)\n",
    "        acc = tf.reduce_mean(res)\n",
    "        sum = tf.reduce_sum(res)\n",
    "        num = res.shape[0]\n",
    "        return num, sum, acc\n",
    "\n",
    "    @tf.function\n",
    "    def train_on_batch(self, x, y):\n",
    "        keras.backend.set_learning_phase(1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model(x)\n",
    "            loss = tf.math.reduce_mean(\n",
    "                keras.losses.categorical_crossentropy(\n",
    "                    y_true=y,\n",
    "                    y_pred=y_pred,\n",
    "                    label_smoothing=self.label_smoothing))\n",
    "            if len(self.model.losses) == 0:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                regularization_loss = tf.math.add_n(self.model.losses)\n",
    "                total_loss = loss + regularization_loss\n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(total_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        self.update_summary(loss=loss)\n",
    "        return total_loss, loss, y_pred\n",
    "\n",
    "    def train_on_epoch(self, epochs, epoch, dataset, show_every_steps):\n",
    "        total_n = 0\n",
    "        total_s = 0\n",
    "        losses = list()\n",
    "        stats = dict()\n",
    "        pbar = tqdm(dataset)\n",
    "        pbar.set_description('Epoch %03d / %03d' % (epoch, epochs))\n",
    "        stats['lr'] = self.optimizer.learning_rate.numpy()\n",
    "        for x, y in pbar:\n",
    "            total_loss, loss, y_pred = self.train_on_batch(x, y)\n",
    "            n, s, acc = self.accuracy(y_true=y, y_pred=y_pred)\n",
    "            total_n += n.numpy()\n",
    "            total_s += s.numpy()\n",
    "            stats['iterations'] = self.optimizer.iterations.numpy()\n",
    "            stats['loss'] = loss.numpy()\n",
    "            stats['total loss'] = total_loss.numpy()\n",
    "            losses.append(total_loss.numpy())\n",
    "            if self.optimizer.iterations % show_every_steps == 0:\n",
    "                avg_acc = total_s / (total_n + 1e-5)\n",
    "                avg_loss = np.mean(losses)\n",
    "                stats['avg loss'] = avg_loss\n",
    "                stats['train acc'] = avg_acc\n",
    "                self.update_summary(avg_acc=avg_acc, avg_loss=avg_loss)\n",
    "            pbar.set_postfix(stats)\n",
    "        pbar.close()\n",
    "\n",
    "    @tf.function\n",
    "    def val_on_batch(self, x):\n",
    "        keras.backend.set_learning_phase(0)\n",
    "        y_pred = self.model(x)\n",
    "        return y_pred\n",
    "\n",
    "    def val_on_epoch(self, epochs, epoch, dataset):\n",
    "        total_n = 0\n",
    "        total_s = 0\n",
    "        avg_acc = 0\n",
    "        pbar = tqdm(dataset)\n",
    "        pbar.set_description('Epoch %03d / %03d' % (epoch, epochs))\n",
    "        for x, y in pbar:\n",
    "            y_pred = self.val_on_batch(x)\n",
    "            n, s, acc = self.accuracy(y_true=y, y_pred=y_pred)\n",
    "            total_n += n.numpy()\n",
    "            total_s += s.numpy()\n",
    "            avg_acc = total_s / (total_n + 1e-5)\n",
    "            pbar.set_postfix({'val acc': avg_acc})\n",
    "        pbar.close()\n",
    "        self.update_summary(val_acc=avg_acc)\n",
    "\n",
    "    def train(self,\n",
    "              train_dataset,\n",
    "              val_dataset,\n",
    "              show_every_steps,\n",
    "              epochs,\n",
    "              start_epoch=0):\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            self.cb_lr(epoch=epoch)\n",
    "            self.train_on_epoch(epochs, epoch, train_dataset, show_every_steps)\n",
    "            self.val_on_epoch(epochs, epoch, val_dataset)\n",
    "            if self.model_dir is None:\n",
    "                pass\n",
    "            else:\n",
    "                self.save_weights(\n",
    "                    os.path.join(self.model_dir, 'snapshot-%d.h5') % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T14:20:29.027360Z",
     "start_time": "2020-04-09T14:20:29.021226Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_generator = generator(dir=train_dir,\n",
    "                                num_classes=num_classes,\n",
    "                                is_shuffle=True,\n",
    "                                is_horizontal_flip=True,\n",
    "                                is_random_crop=True,\n",
    "                                is_random_cutout=True)\n",
    "    train_generator = tf.data.Dataset.from_generator(train_generator,\n",
    "                                                     output_types=(tf.float32,\n",
    "                                                                   tf.int32))\n",
    "    train_generator = train_generator.batch(batch_size=batch_size)\n",
    "    train_generator = train_generator.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    val_generator = generator(dir=test_dir, num_classes=num_classes)\n",
    "    val_generator = tf.data.Dataset.from_generator(val_generator,\n",
    "                                                   output_types=(tf.float32,\n",
    "                                                                 tf.int32))\n",
    "    val_generator = val_generator.batch(batch_size=batch_size)\n",
    "    val_generator = val_generator.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model = MobileNetV2(input_shape=(height, width, num_channels),\n",
    "                        num_classes=num_classes,\n",
    "                        weight_decay=weight_decay)\n",
    "    trainer = Trainer(model_dir=model_dir,\n",
    "                      summary_dir=summary_dir,\n",
    "                      model=model,\n",
    "                      optimizer=keras.optimizers.SGD(learning_rate=lr, momentum=0.9),\n",
    "                      lr_range=lr_range,\n",
    "                      label_smoothing=label_smoothing)\n",
    "    trainer.train(train_dataset=train_generator,\n",
    "                  val_dataset=val_generator,\n",
    "                  show_every_steps=show_every_steps,\n",
    "                  epochs=epochs,\n",
    "                  start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:05:30.793886Z",
     "start_time": "2020-04-09T14:20:29.368708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 000 / 400: : 643it [01:42,  6.29it/s, lr=0.1, iterations=643, loss=1.86, total loss=3.83, avg loss=4.58, train acc=0.377]\n",
      "Epoch 000 / 400: : 157it [00:06, 25.41it/s, val acc=0.404]\n",
      "Epoch 001 / 400: : 643it [01:33,  6.86it/s, lr=0.1, iterations=1286, loss=1.54, total loss=2.92, avg loss=3.35, train acc=0.545]\n",
      "Epoch 001 / 400: : 157it [00:05, 29.63it/s, val acc=0.59] \n",
      "Epoch 002 / 400: : 643it [01:34,  6.82it/s, lr=0.1, iterations=1929, loss=1.57, total loss=2.64, avg loss=2.76, train acc=0.628]\n",
      "Epoch 002 / 400: : 157it [00:06, 25.91it/s, val acc=0.664]\n",
      "Epoch 003 / 400: : 643it [01:34,  6.81it/s, lr=0.1, iterations=2572, loss=1.44, total loss=2.3, avg loss=2.42, train acc=0.684] \n",
      "Epoch 003 / 400: : 157it [00:04, 34.21it/s, val acc=0.684]\n",
      "Epoch 004 / 400: : 643it [01:34,  6.77it/s, lr=0.1, iterations=3215, loss=1.42, total loss=2.13, avg loss=2.16, train acc=0.727]\n",
      "Epoch 004 / 400: : 157it [00:04, 33.94it/s, val acc=0.729]\n",
      "Epoch 005 / 400: : 643it [01:34,  6.79it/s, lr=0.1, iterations=3858, loss=1.22, total loss=1.81, avg loss=2.01, train acc=0.741]\n",
      "Epoch 005 / 400: : 157it [00:04, 31.46it/s, val acc=0.73] \n",
      "Epoch 006 / 400: : 643it [01:34,  6.78it/s, lr=0.1, iterations=4501, loss=1.38, total loss=1.89, avg loss=1.86, train acc=0.763]\n",
      "Epoch 006 / 400: : 157it [00:04, 32.75it/s, val acc=0.708]\n",
      "Epoch 007 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=5144, loss=1.38, total loss=1.83, avg loss=1.78, train acc=0.773]\n",
      "Epoch 007 / 400: : 157it [00:05, 27.00it/s, val acc=0.705]\n",
      "Epoch 008 / 400: : 643it [01:35,  6.74it/s, lr=0.1, iterations=5787, loss=1.28, total loss=1.68, avg loss=1.7, train acc=0.786] \n",
      "Epoch 008 / 400: : 157it [00:05, 29.00it/s, val acc=0.746]\n",
      "Epoch 009 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=6430, loss=1.39, total loss=1.75, avg loss=1.64, train acc=0.791]\n",
      "Epoch 009 / 400: : 157it [00:05, 28.68it/s, val acc=0.752]\n",
      "Epoch 010 / 400: : 643it [01:34,  6.78it/s, lr=0.1, iterations=7073, loss=1.3, total loss=1.63, avg loss=1.61, train acc=0.792] \n",
      "Epoch 010 / 400: : 157it [00:04, 33.21it/s, val acc=0.795]\n",
      "Epoch 011 / 400: : 643it [01:35,  6.74it/s, lr=0.1, iterations=7716, loss=1.2, total loss=1.51, avg loss=1.57, train acc=0.802] \n",
      "Epoch 011 / 400: : 157it [00:05, 31.11it/s, val acc=0.765]\n",
      "Epoch 012 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=8359, loss=1.32, total loss=1.61, avg loss=1.54, train acc=0.806]\n",
      "Epoch 012 / 400: : 157it [00:05, 28.11it/s, val acc=0.773]\n",
      "Epoch 013 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=9002, loss=1.36, total loss=1.64, avg loss=1.52, train acc=0.811]\n",
      "Epoch 013 / 400: : 157it [00:04, 32.98it/s, val acc=0.778]\n",
      "Epoch 014 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=9645, loss=1.32, total loss=1.6, avg loss=1.51, train acc=0.813] \n",
      "Epoch 014 / 400: : 157it [00:05, 26.88it/s, val acc=0.776]\n",
      "Epoch 015 / 400: : 643it [01:35,  6.76it/s, lr=0.1, iterations=10288, loss=1.17, total loss=1.44, avg loss=1.49, train acc=0.816]\n",
      "Epoch 015 / 400: : 157it [00:04, 32.11it/s, val acc=0.786]\n",
      "Epoch 016 / 400: : 643it [01:35,  6.73it/s, lr=0.1, iterations=10931, loss=1.28, total loss=1.55, avg loss=1.48, train acc=0.819]\n",
      "Epoch 016 / 400: : 157it [00:06, 24.55it/s, val acc=0.794]\n",
      "Epoch 017 / 400: : 643it [01:35,  6.77it/s, lr=0.1, iterations=11574, loss=1.33, total loss=1.6, avg loss=1.47, train acc=0.824] \n",
      "Epoch 017 / 400: : 157it [00:04, 33.12it/s, val acc=0.764]\n",
      "Epoch 018 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=12217, loss=1.23, total loss=1.49, avg loss=1.47, train acc=0.825]\n",
      "Epoch 018 / 400: : 157it [00:04, 33.09it/s, val acc=0.749]\n",
      "Epoch 019 / 400: : 643it [01:35,  6.76it/s, lr=0.1, iterations=12860, loss=1.24, total loss=1.5, avg loss=1.46, train acc=0.827]  \n",
      "Epoch 019 / 400: : 157it [00:06, 23.08it/s, val acc=0.818]\n",
      "Epoch 020 / 400: : 643it [01:35,  6.76it/s, lr=0.1, iterations=13503, loss=1.34, total loss=1.59, avg loss=1.45, train acc=0.829] \n",
      "Epoch 020 / 400: : 157it [00:05, 28.11it/s, val acc=0.743]\n",
      "Epoch 021 / 400: : 643it [01:34,  6.77it/s, lr=0.1, iterations=14146, loss=1.23, total loss=1.48, avg loss=1.45, train acc=0.83] \n",
      "Epoch 021 / 400: : 157it [00:04, 32.83it/s, val acc=0.829]\n",
      "Epoch 022 / 400: : 643it [01:35,  6.75it/s, lr=0.1, iterations=14789, loss=1.19, total loss=1.44, avg loss=1.44, train acc=0.832]\n",
      "Epoch 022 / 400: : 157it [00:04, 32.78it/s, val acc=0.816]\n",
      "Epoch 023 / 400: : 643it [01:34,  6.84it/s, lr=0.1, iterations=15432, loss=1.25, total loss=1.51, avg loss=1.44, train acc=0.834] \n",
      "Epoch 023 / 400: : 157it [00:08, 19.03it/s, val acc=0.807]\n",
      "Epoch 024 / 400: : 643it [01:34,  6.83it/s, lr=0.1, iterations=16075, loss=1.15, total loss=1.41, avg loss=1.44, train acc=0.836] \n",
      "Epoch 024 / 400: : 157it [00:04, 31.64it/s, val acc=0.792]\n",
      "Epoch 025 / 400: : 643it [01:34,  6.79it/s, lr=0.1, iterations=16718, loss=1.16, total loss=1.42, avg loss=1.44, train acc=0.835]\n",
      "Epoch 025 / 400: : 157it [00:05, 30.39it/s, val acc=0.786]\n",
      "Epoch 026 / 400: : 643it [01:32,  6.92it/s, lr=0.1, iterations=17361, loss=1.44, total loss=1.69, avg loss=1.43, train acc=0.84]  \n",
      "Epoch 026 / 400: : 157it [00:04, 33.27it/s, val acc=0.811]\n",
      "Epoch 027 / 400: : 643it [01:33,  6.86it/s, lr=0.1, iterations=18004, loss=1.2, total loss=1.45, avg loss=1.43, train acc=0.842] \n",
      "Epoch 027 / 400: : 157it [00:04, 33.24it/s, val acc=0.783]\n",
      "Epoch 028 / 400: : 643it [01:33,  6.86it/s, lr=0.1, iterations=18647, loss=1.18, total loss=1.43, avg loss=1.43, train acc=0.839] \n",
      "Epoch 028 / 400: : 157it [00:05, 31.30it/s, val acc=0.815]\n",
      "Epoch 029 / 400: : 643it [01:34,  6.84it/s, lr=0.1, iterations=19290, loss=1.42, total loss=1.67, avg loss=1.42, train acc=0.844]\n",
      "Epoch 029 / 400: : 157it [00:04, 33.28it/s, val acc=0.818]\n",
      "Epoch 030 / 400: : 643it [01:33,  6.88it/s, lr=0.1, iterations=19933, loss=1.38, total loss=1.64, avg loss=1.43, train acc=0.843] \n",
      "Epoch 030 / 400: : 157it [00:04, 33.04it/s, val acc=0.84] \n",
      "Epoch 031 / 400: : 643it [01:33,  6.87it/s, lr=0.1, iterations=20576, loss=1.17, total loss=1.42, avg loss=1.43, train acc=0.84] \n",
      "Epoch 031 / 400: : 157it [00:04, 33.21it/s, val acc=0.452]\n",
      "Epoch 032 / 400: : 643it [01:33,  6.88it/s, lr=0.1, iterations=21219, loss=1.07, total loss=1.33, avg loss=1.43, train acc=0.843] \n",
      "Epoch 032 / 400: : 157it [00:04, 31.96it/s, val acc=0.82] \n",
      "Epoch 033 / 400: : 643it [01:33,  6.89it/s, lr=0.1, iterations=21862, loss=1.24, total loss=1.49, avg loss=1.43, train acc=0.844]\n",
      "Epoch 033 / 400: : 157it [00:05, 30.75it/s, val acc=0.778]\n",
      "Epoch 034 / 400: : 643it [01:33,  6.88it/s, lr=0.1, iterations=22505, loss=1.15, total loss=1.41, avg loss=1.42, train acc=0.846] \n",
      "Epoch 034 / 400: : 157it [00:04, 31.83it/s, val acc=0.824]\n",
      "Epoch 035 / 400: : 643it [01:33,  6.90it/s, lr=0.1, iterations=23148, loss=1.07, total loss=1.33, avg loss=1.43, train acc=0.844]\n",
      "Epoch 035 / 400: : 157it [00:04, 33.52it/s, val acc=0.82] \n",
      "Epoch 036 / 400: : 643it [01:33,  6.90it/s, lr=0.1, iterations=23791, loss=1.12, total loss=1.37, avg loss=1.42, train acc=0.847]\n",
      "Epoch 036 / 400: : 157it [00:04, 33.04it/s, val acc=0.803]\n",
      "Epoch 037 / 400: : 643it [01:32,  6.96it/s, lr=0.1, iterations=24434, loss=1.05, total loss=1.31, avg loss=1.42, train acc=0.845]\n",
      "Epoch 037 / 400: : 157it [00:04, 33.17it/s, val acc=0.787]\n",
      "Epoch 038 / 400: : 643it [01:33,  6.90it/s, lr=0.1, iterations=25077, loss=1.06, total loss=1.31, avg loss=1.41, train acc=0.849] \n",
      "Epoch 038 / 400: : 157it [00:04, 33.25it/s, val acc=0.815]\n",
      "Epoch 039 / 400: : 643it [01:32,  6.94it/s, lr=0.1, iterations=25720, loss=1.19, total loss=1.45, avg loss=1.42, train acc=0.849] \n",
      "Epoch 039 / 400: : 157it [00:04, 32.03it/s, val acc=0.831]\n",
      "Epoch 040 / 400: : 643it [01:32,  6.95it/s, lr=0.1, iterations=26363, loss=1.16, total loss=1.42, avg loss=1.42, train acc=0.848]\n",
      "Epoch 040 / 400: : 157it [00:04, 33.28it/s, val acc=0.81] \n",
      "Epoch 041 / 400: : 643it [01:33,  6.89it/s, lr=0.1, iterations=27006, loss=1.25, total loss=1.52, avg loss=1.43, train acc=0.845] \n",
      "Epoch 041 / 400: : 157it [00:04, 31.72it/s, val acc=0.84] \n",
      "Epoch 042 / 400: : 643it [01:33,  6.91it/s, lr=0.1, iterations=27649, loss=1.11, total loss=1.38, avg loss=1.42, train acc=0.851]\n",
      "Epoch 042 / 400: : 157it [00:04, 32.54it/s, val acc=0.829]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 043 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=28292, loss=1.22, total loss=1.49, avg loss=1.41, train acc=0.853] \n",
      "Epoch 043 / 400: : 157it [00:04, 33.84it/s, val acc=0.807]\n",
      "Epoch 044 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=28935, loss=1.2, total loss=1.46, avg loss=1.42, train acc=0.852] \n",
      "Epoch 044 / 400: : 157it [00:04, 33.89it/s, val acc=0.824]\n",
      "Epoch 045 / 400: : 643it [01:30,  7.13it/s, lr=0.1, iterations=29578, loss=1.09, total loss=1.35, avg loss=1.41, train acc=0.853] \n",
      "Epoch 045 / 400: : 157it [00:04, 33.74it/s, val acc=0.821]\n",
      "Epoch 046 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=30221, loss=1.21, total loss=1.47, avg loss=1.41, train acc=0.852] \n",
      "Epoch 046 / 400: : 157it [00:04, 33.59it/s, val acc=0.822]\n",
      "Epoch 047 / 400: : 643it [01:30,  7.14it/s, lr=0.1, iterations=30864, loss=1.15, total loss=1.42, avg loss=1.41, train acc=0.852] \n",
      "Epoch 047 / 400: : 157it [00:04, 33.62it/s, val acc=0.809]\n",
      "Epoch 048 / 400: : 643it [01:30,  7.14it/s, lr=0.1, iterations=31507, loss=1.28, total loss=1.54, avg loss=1.41, train acc=0.855] \n",
      "Epoch 048 / 400: : 157it [00:04, 33.78it/s, val acc=0.805]\n",
      "Epoch 049 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=32150, loss=1.31, total loss=1.57, avg loss=1.41, train acc=0.853] \n",
      "Epoch 049 / 400: : 157it [00:04, 33.55it/s, val acc=0.843]\n",
      "Epoch 050 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=32793, loss=1.11, total loss=1.38, avg loss=1.41, train acc=0.852] \n",
      "Epoch 050 / 400: : 157it [00:04, 33.55it/s, val acc=0.806]\n",
      "Epoch 051 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=33436, loss=1.38, total loss=1.64, avg loss=1.41, train acc=0.854] \n",
      "Epoch 051 / 400: : 157it [00:04, 33.81it/s, val acc=0.782]\n",
      "Epoch 052 / 400: : 643it [01:31,  7.07it/s, lr=0.1, iterations=34079, loss=1.05, total loss=1.32, avg loss=1.41, train acc=0.858] \n",
      "Epoch 052 / 400: : 157it [00:04, 33.83it/s, val acc=0.82] \n",
      "Epoch 053 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=34722, loss=1.32, total loss=1.59, avg loss=1.41, train acc=0.857] \n",
      "Epoch 053 / 400: : 157it [00:04, 33.98it/s, val acc=0.826]\n",
      "Epoch 054 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=35365, loss=1.17, total loss=1.44, avg loss=1.41, train acc=0.857] \n",
      "Epoch 054 / 400: : 157it [00:04, 33.21it/s, val acc=0.757]\n",
      "Epoch 055 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=36008, loss=1.2, total loss=1.47, avg loss=1.41, train acc=0.859]  \n",
      "Epoch 055 / 400: : 157it [00:04, 33.64it/s, val acc=0.714]\n",
      "Epoch 056 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=36651, loss=1.14, total loss=1.41, avg loss=1.41, train acc=0.857] \n",
      "Epoch 056 / 400: : 157it [00:04, 33.73it/s, val acc=0.786]\n",
      "Epoch 057 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=37294, loss=1.19, total loss=1.46, avg loss=1.41, train acc=0.854] \n",
      "Epoch 057 / 400: : 157it [00:04, 33.56it/s, val acc=0.821]\n",
      "Epoch 058 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=37937, loss=1.09, total loss=1.35, avg loss=1.41, train acc=0.858]\n",
      "Epoch 058 / 400: : 157it [00:04, 33.92it/s, val acc=0.818]\n",
      "Epoch 059 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=38580, loss=1.15, total loss=1.42, avg loss=1.4, train acc=0.859]  \n",
      "Epoch 059 / 400: : 157it [00:04, 31.58it/s, val acc=0.817]\n",
      "Epoch 060 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=39223, loss=1.3, total loss=1.56, avg loss=1.41, train acc=0.857]  \n",
      "Epoch 060 / 400: : 157it [00:04, 33.94it/s, val acc=0.817]\n",
      "Epoch 061 / 400: : 643it [01:31,  7.02it/s, lr=0.1, iterations=39866, loss=1.18, total loss=1.45, avg loss=1.41, train acc=0.857] \n",
      "Epoch 061 / 400: : 157it [00:04, 33.81it/s, val acc=0.819]\n",
      "Epoch 062 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=40509, loss=1.19, total loss=1.46, avg loss=1.41, train acc=0.857]\n",
      "Epoch 062 / 400: : 157it [00:04, 33.85it/s, val acc=0.829]\n",
      "Epoch 063 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=41152, loss=1.25, total loss=1.52, avg loss=1.41, train acc=0.858] \n",
      "Epoch 063 / 400: : 157it [00:04, 33.81it/s, val acc=0.804]\n",
      "Epoch 064 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=41795, loss=0.984, total loss=1.25, avg loss=1.41, train acc=0.86] \n",
      "Epoch 064 / 400: : 157it [00:04, 33.75it/s, val acc=0.829]\n",
      "Epoch 065 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=42438, loss=1.22, total loss=1.49, avg loss=1.41, train acc=0.86]  \n",
      "Epoch 065 / 400: : 157it [00:04, 33.85it/s, val acc=0.815]\n",
      "Epoch 066 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=43081, loss=1.2, total loss=1.47, avg loss=1.4, train acc=0.86]    \n",
      "Epoch 066 / 400: : 157it [00:04, 33.86it/s, val acc=0.849]\n",
      "Epoch 067 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=43724, loss=1.26, total loss=1.53, avg loss=1.4, train acc=0.861]  \n",
      "Epoch 067 / 400: : 157it [00:04, 33.39it/s, val acc=0.762]\n",
      "Epoch 068 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=44367, loss=1.09, total loss=1.36, avg loss=1.41, train acc=0.856] \n",
      "Epoch 068 / 400: : 157it [00:04, 33.95it/s, val acc=0.839]\n",
      "Epoch 069 / 400: : 643it [01:31,  7.01it/s, lr=0.1, iterations=45010, loss=1.31, total loss=1.58, avg loss=1.41, train acc=0.86]  \n",
      "Epoch 069 / 400: : 157it [00:04, 33.45it/s, val acc=0.82] \n",
      "Epoch 070 / 400: : 643it [01:31,  7.04it/s, lr=0.1, iterations=45653, loss=1.13, total loss=1.4, avg loss=1.41, train acc=0.859]  \n",
      "Epoch 070 / 400: : 157it [00:04, 33.67it/s, val acc=0.834]\n",
      "Epoch 071 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=46296, loss=1.1, total loss=1.37, avg loss=1.41, train acc=0.858]  \n",
      "Epoch 071 / 400: : 157it [00:04, 33.88it/s, val acc=0.825]\n",
      "Epoch 072 / 400: : 643it [01:31,  7.03it/s, lr=0.1, iterations=46939, loss=1.14, total loss=1.42, avg loss=1.4, train acc=0.863]  \n",
      "Epoch 072 / 400: : 157it [00:04, 33.56it/s, val acc=0.793]\n",
      "Epoch 073 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=47582, loss=1.26, total loss=1.53, avg loss=1.41, train acc=0.86]  \n",
      "Epoch 073 / 400: : 157it [00:04, 33.53it/s, val acc=0.807]\n",
      "Epoch 074 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=48225, loss=1.02, total loss=1.3, avg loss=1.41, train acc=0.861] \n",
      "Epoch 074 / 400: : 157it [00:04, 33.60it/s, val acc=0.812]\n",
      "Epoch 075 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=48868, loss=1.07, total loss=1.34, avg loss=1.42, train acc=0.856] \n",
      "Epoch 075 / 400: : 157it [00:04, 33.87it/s, val acc=0.848]\n",
      "Epoch 076 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=49511, loss=1.29, total loss=1.57, avg loss=1.41, train acc=0.861] \n",
      "Epoch 076 / 400: : 157it [00:04, 32.84it/s, val acc=0.832]\n",
      "Epoch 077 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=50154, loss=1.12, total loss=1.4, avg loss=1.41, train acc=0.859]  \n",
      "Epoch 077 / 400: : 157it [00:04, 33.63it/s, val acc=0.791]\n",
      "Epoch 078 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=50797, loss=1.13, total loss=1.4, avg loss=1.41, train acc=0.862]  \n",
      "Epoch 078 / 400: : 157it [00:04, 33.76it/s, val acc=0.844]\n",
      "Epoch 079 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=51440, loss=1.22, total loss=1.49, avg loss=1.41, train acc=0.86]  \n",
      "Epoch 079 / 400: : 157it [00:04, 33.95it/s, val acc=0.828]\n",
      "Epoch 080 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=52083, loss=1.07, total loss=1.35, avg loss=1.41, train acc=0.864] \n",
      "Epoch 080 / 400: : 157it [00:04, 33.77it/s, val acc=0.843]\n",
      "Epoch 081 / 400: : 643it [01:31,  7.01it/s, lr=0.1, iterations=52726, loss=1.01, total loss=1.29, avg loss=1.41, train acc=0.861] \n",
      "Epoch 081 / 400: : 157it [00:04, 33.80it/s, val acc=0.761]\n",
      "Epoch 082 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=53369, loss=1.15, total loss=1.43, avg loss=1.41, train acc=0.86]  \n",
      "Epoch 082 / 400: : 157it [00:04, 31.65it/s, val acc=0.79] \n",
      "Epoch 083 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=54012, loss=1.22, total loss=1.5, avg loss=1.41, train acc=0.861]  \n",
      "Epoch 083 / 400: : 157it [00:04, 31.83it/s, val acc=0.72] \n",
      "Epoch 084 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=54655, loss=1.13, total loss=1.41, avg loss=1.4, train acc=0.865]  \n",
      "Epoch 084 / 400: : 157it [00:04, 33.91it/s, val acc=0.79] \n",
      "Epoch 085 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=55298, loss=1.15, total loss=1.43, avg loss=1.41, train acc=0.863] \n",
      "Epoch 085 / 400: : 157it [00:04, 33.84it/s, val acc=0.813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 086 / 400: : 643it [01:30,  7.13it/s, lr=0.1, iterations=55941, loss=1.18, total loss=1.46, avg loss=1.4, train acc=0.864]  \n",
      "Epoch 086 / 400: : 157it [00:04, 33.93it/s, val acc=0.813]\n",
      "Epoch 087 / 400: : 643it [01:28,  7.25it/s, lr=0.1, iterations=56584, loss=1.3, total loss=1.58, avg loss=1.41, train acc=0.863]  \n",
      "Epoch 087 / 400: : 157it [00:04, 33.59it/s, val acc=0.844]\n",
      "Epoch 088 / 400: : 643it [01:30,  7.13it/s, lr=0.1, iterations=57227, loss=1.22, total loss=1.5, avg loss=1.41, train acc=0.861]  \n",
      "Epoch 088 / 400: : 157it [00:04, 33.82it/s, val acc=0.831]\n",
      "Epoch 089 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=57870, loss=1.4, total loss=1.67, avg loss=1.41, train acc=0.863]  \n",
      "Epoch 089 / 400: : 157it [00:04, 33.86it/s, val acc=0.783]\n",
      "Epoch 090 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=58513, loss=1.05, total loss=1.32, avg loss=1.41, train acc=0.862]\n",
      "Epoch 090 / 400: : 157it [00:04, 33.89it/s, val acc=0.81] \n",
      "Epoch 091 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=59156, loss=1.04, total loss=1.31, avg loss=1.41, train acc=0.862] \n",
      "Epoch 091 / 400: : 157it [00:04, 33.94it/s, val acc=0.799]\n",
      "Epoch 092 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=59799, loss=1.23, total loss=1.51, avg loss=1.41, train acc=0.864] \n",
      "Epoch 092 / 400: : 157it [00:04, 33.67it/s, val acc=0.746]\n",
      "Epoch 093 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=60442, loss=1.03, total loss=1.31, avg loss=1.41, train acc=0.864] \n",
      "Epoch 093 / 400: : 157it [00:04, 33.58it/s, val acc=0.844]\n",
      "Epoch 094 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=61085, loss=1.1, total loss=1.38, avg loss=1.41, train acc=0.864]  \n",
      "Epoch 094 / 400: : 157it [00:04, 34.02it/s, val acc=0.841]\n",
      "Epoch 095 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=61728, loss=1.2, total loss=1.48, avg loss=1.4, train acc=0.867]   \n",
      "Epoch 095 / 400: : 157it [00:04, 33.65it/s, val acc=0.783]\n",
      "Epoch 096 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=62371, loss=1.24, total loss=1.52, avg loss=1.4, train acc=0.866]  \n",
      "Epoch 096 / 400: : 157it [00:04, 33.54it/s, val acc=0.822]\n",
      "Epoch 097 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=63014, loss=1.2, total loss=1.48, avg loss=1.41, train acc=0.863]  \n",
      "Epoch 097 / 400: : 157it [00:04, 33.69it/s, val acc=0.847]\n",
      "Epoch 098 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=63657, loss=1.09, total loss=1.37, avg loss=1.41, train acc=0.863] \n",
      "Epoch 098 / 400: : 157it [00:04, 33.82it/s, val acc=0.835]\n",
      "Epoch 099 / 400: : 643it [01:31,  7.07it/s, lr=0.1, iterations=64300, loss=1.23, total loss=1.51, avg loss=1.41, train acc=0.867] \n",
      "Epoch 099 / 400: : 157it [00:04, 33.71it/s, val acc=0.803]\n",
      "Epoch 100 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=64943, loss=1.28, total loss=1.56, avg loss=1.41, train acc=0.863] \n",
      "Epoch 100 / 400: : 157it [00:04, 33.34it/s, val acc=0.853]\n",
      "Epoch 101 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=65586, loss=1.17, total loss=1.45, avg loss=1.41, train acc=0.866] \n",
      "Epoch 101 / 400: : 157it [00:04, 33.75it/s, val acc=0.82] \n",
      "Epoch 102 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=66229, loss=1.1, total loss=1.38, avg loss=1.41, train acc=0.864]  \n",
      "Epoch 102 / 400: : 157it [00:04, 33.76it/s, val acc=0.796]\n",
      "Epoch 103 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=66872, loss=1.18, total loss=1.46, avg loss=1.41, train acc=0.863] \n",
      "Epoch 103 / 400: : 157it [00:04, 33.36it/s, val acc=0.83] \n",
      "Epoch 104 / 400: : 643it [01:31,  7.04it/s, lr=0.1, iterations=67515, loss=1.16, total loss=1.44, avg loss=1.41, train acc=0.865] \n",
      "Epoch 104 / 400: : 157it [00:04, 33.81it/s, val acc=0.799]\n",
      "Epoch 105 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=68158, loss=1.2, total loss=1.48, avg loss=1.41, train acc=0.864]  \n",
      "Epoch 105 / 400: : 157it [00:04, 33.59it/s, val acc=0.826]\n",
      "Epoch 106 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=68801, loss=1.09, total loss=1.37, avg loss=1.41, train acc=0.864] \n",
      "Epoch 106 / 400: : 157it [00:04, 33.79it/s, val acc=0.836]\n",
      "Epoch 107 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=69444, loss=1.04, total loss=1.32, avg loss=1.41, train acc=0.866] \n",
      "Epoch 107 / 400: : 157it [00:04, 33.73it/s, val acc=0.855]\n",
      "Epoch 108 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=70087, loss=1.08, total loss=1.36, avg loss=1.4, train acc=0.866] \n",
      "Epoch 108 / 400: : 157it [00:04, 33.99it/s, val acc=0.849]\n",
      "Epoch 109 / 400: : 643it [01:31,  7.02it/s, lr=0.1, iterations=70730, loss=1.1, total loss=1.38, avg loss=1.41, train acc=0.861]  \n",
      "Epoch 109 / 400: : 157it [00:04, 33.50it/s, val acc=0.822]\n",
      "Epoch 110 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=71373, loss=1.35, total loss=1.63, avg loss=1.41, train acc=0.862] \n",
      "Epoch 110 / 400: : 157it [00:04, 33.75it/s, val acc=0.794]\n",
      "Epoch 111 / 400: : 643it [01:31,  7.03it/s, lr=0.1, iterations=72016, loss=1.11, total loss=1.4, avg loss=1.41, train acc=0.868]  \n",
      "Epoch 111 / 400: : 157it [00:04, 33.72it/s, val acc=0.824]\n",
      "Epoch 112 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=72659, loss=1.12, total loss=1.4, avg loss=1.41, train acc=0.866]  \n",
      "Epoch 112 / 400: : 157it [00:04, 32.83it/s, val acc=0.837]\n",
      "Epoch 113 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=73302, loss=1.16, total loss=1.44, avg loss=1.41, train acc=0.862] \n",
      "Epoch 113 / 400: : 157it [00:04, 34.04it/s, val acc=0.837]\n",
      "Epoch 114 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=73945, loss=1.07, total loss=1.35, avg loss=1.4, train acc=0.866]  \n",
      "Epoch 114 / 400: : 157it [00:04, 33.68it/s, val acc=0.792]\n",
      "Epoch 115 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=74588, loss=1.07, total loss=1.35, avg loss=1.4, train acc=0.868]  \n",
      "Epoch 115 / 400: : 157it [00:04, 33.86it/s, val acc=0.839]\n",
      "Epoch 116 / 400: : 643it [01:31,  7.05it/s, lr=0.1, iterations=75231, loss=1.26, total loss=1.54, avg loss=1.41, train acc=0.867] \n",
      "Epoch 116 / 400: : 157it [00:04, 33.90it/s, val acc=0.857]\n",
      "Epoch 117 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=75874, loss=1.13, total loss=1.41, avg loss=1.41, train acc=0.867] \n",
      "Epoch 117 / 400: : 157it [00:04, 33.82it/s, val acc=0.818]\n",
      "Epoch 118 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=76517, loss=1.13, total loss=1.41, avg loss=1.41, train acc=0.866] \n",
      "Epoch 118 / 400: : 157it [00:04, 33.55it/s, val acc=0.812]\n",
      "Epoch 119 / 400: : 643it [01:31,  7.02it/s, lr=0.1, iterations=77160, loss=1.12, total loss=1.41, avg loss=1.41, train acc=0.864] \n",
      "Epoch 119 / 400: : 157it [00:04, 33.82it/s, val acc=0.815]\n",
      "Epoch 120 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=77803, loss=1.11, total loss=1.39, avg loss=1.41, train acc=0.865] \n",
      "Epoch 120 / 400: : 157it [00:04, 33.93it/s, val acc=0.842]\n",
      "Epoch 121 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=78446, loss=1.2, total loss=1.49, avg loss=1.41, train acc=0.867]  \n",
      "Epoch 121 / 400: : 157it [00:04, 33.85it/s, val acc=0.79] \n",
      "Epoch 122 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=79089, loss=1.22, total loss=1.5, avg loss=1.41, train acc=0.863]  \n",
      "Epoch 122 / 400: : 157it [00:04, 33.49it/s, val acc=0.817]\n",
      "Epoch 123 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=79732, loss=1.11, total loss=1.39, avg loss=1.41, train acc=0.868] \n",
      "Epoch 123 / 400: : 157it [00:04, 33.84it/s, val acc=0.817]\n",
      "Epoch 124 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=80375, loss=1.12, total loss=1.4, avg loss=1.4, train acc=0.869]   \n",
      "Epoch 124 / 400: : 157it [00:04, 33.61it/s, val acc=0.785]\n",
      "Epoch 125 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=81018, loss=1.11, total loss=1.39, avg loss=1.41, train acc=0.864] \n",
      "Epoch 125 / 400: : 157it [00:04, 33.80it/s, val acc=0.825]\n",
      "Epoch 126 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=81661, loss=1.25, total loss=1.53, avg loss=1.41, train acc=0.867] \n",
      "Epoch 126 / 400: : 157it [00:04, 33.63it/s, val acc=0.811]\n",
      "Epoch 127 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=82304, loss=1.03, total loss=1.31, avg loss=1.41, train acc=0.867] \n",
      "Epoch 127 / 400: : 157it [00:04, 33.75it/s, val acc=0.843]\n",
      "Epoch 128 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=82947, loss=1.16, total loss=1.45, avg loss=1.41, train acc=0.866] \n",
      "Epoch 128 / 400: : 157it [00:04, 33.22it/s, val acc=0.839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 400: : 643it [01:30,  7.14it/s, lr=0.1, iterations=83590, loss=1.31, total loss=1.59, avg loss=1.4, train acc=0.868]  \n",
      "Epoch 129 / 400: : 157it [00:04, 33.50it/s, val acc=0.781]\n",
      "Epoch 130 / 400: : 643it [01:30,  7.14it/s, lr=0.1, iterations=84233, loss=1.26, total loss=1.54, avg loss=1.41, train acc=0.869] \n",
      "Epoch 130 / 400: : 157it [00:04, 33.53it/s, val acc=0.842]\n",
      "Epoch 131 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=84876, loss=1.12, total loss=1.41, avg loss=1.41, train acc=0.866] \n",
      "Epoch 131 / 400: : 157it [00:04, 33.84it/s, val acc=0.84] \n",
      "Epoch 132 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=85519, loss=1.18, total loss=1.47, avg loss=1.41, train acc=0.866] \n",
      "Epoch 132 / 400: : 157it [00:04, 33.67it/s, val acc=0.813]\n",
      "Epoch 133 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=86162, loss=1.23, total loss=1.51, avg loss=1.41, train acc=0.867] \n",
      "Epoch 133 / 400: : 157it [00:04, 33.93it/s, val acc=0.84] \n",
      "Epoch 134 / 400: : 643it [01:30,  7.13it/s, lr=0.1, iterations=86805, loss=1.15, total loss=1.44, avg loss=1.41, train acc=0.868] \n",
      "Epoch 134 / 400: : 157it [00:04, 33.91it/s, val acc=0.847]\n",
      "Epoch 135 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=87448, loss=1.07, total loss=1.36, avg loss=1.41, train acc=0.866] \n",
      "Epoch 135 / 400: : 157it [00:04, 33.61it/s, val acc=0.854]\n",
      "Epoch 136 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=88091, loss=1.13, total loss=1.42, avg loss=1.41, train acc=0.866] \n",
      "Epoch 136 / 400: : 157it [00:04, 33.51it/s, val acc=0.818]\n",
      "Epoch 137 / 400: : 643it [01:30,  7.10it/s, lr=0.1, iterations=88734, loss=1.22, total loss=1.5, avg loss=1.4, train acc=0.869]   \n",
      "Epoch 137 / 400: : 157it [00:04, 33.85it/s, val acc=0.829]\n",
      "Epoch 138 / 400: : 643it [01:29,  7.15it/s, lr=0.1, iterations=89377, loss=1.17, total loss=1.46, avg loss=1.4, train acc=0.868]  \n",
      "Epoch 138 / 400: : 157it [00:04, 33.78it/s, val acc=0.811]\n",
      "Epoch 139 / 400: : 643it [01:30,  7.07it/s, lr=0.1, iterations=9e+4, loss=1.18, total loss=1.46, avg loss=1.41, train acc=0.867]  \n",
      "Epoch 139 / 400: : 157it [00:04, 33.65it/s, val acc=0.835]\n",
      "Epoch 140 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=90663, loss=1.1, total loss=1.38, avg loss=1.41, train acc=0.866]  \n",
      "Epoch 140 / 400: : 157it [00:04, 34.13it/s, val acc=0.787]\n",
      "Epoch 141 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=91306, loss=1.22, total loss=1.51, avg loss=1.4, train acc=0.869]  \n",
      "Epoch 141 / 400: : 157it [00:04, 33.57it/s, val acc=0.826]\n",
      "Epoch 142 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=91949, loss=1.23, total loss=1.52, avg loss=1.41, train acc=0.867] \n",
      "Epoch 142 / 400: : 157it [00:04, 33.74it/s, val acc=0.828]\n",
      "Epoch 143 / 400: : 643it [01:30,  7.12it/s, lr=0.1, iterations=92592, loss=1.11, total loss=1.4, avg loss=1.41, train acc=0.868]  \n",
      "Epoch 143 / 400: : 157it [00:04, 33.96it/s, val acc=0.756]\n",
      "Epoch 144 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=93235, loss=1.13, total loss=1.42, avg loss=1.41, train acc=0.866] \n",
      "Epoch 144 / 400: : 157it [00:04, 33.78it/s, val acc=0.814]\n",
      "Epoch 145 / 400: : 643it [01:31,  7.06it/s, lr=0.1, iterations=93878, loss=1.1, total loss=1.39, avg loss=1.41, train acc=0.867]  \n",
      "Epoch 145 / 400: : 157it [00:04, 33.73it/s, val acc=0.834]\n",
      "Epoch 146 / 400: : 643it [01:30,  7.11it/s, lr=0.1, iterations=94521, loss=1.12, total loss=1.41, avg loss=1.41, train acc=0.865] \n",
      "Epoch 146 / 400: : 157it [00:04, 33.89it/s, val acc=0.813]\n",
      "Epoch 147 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=95164, loss=1.05, total loss=1.34, avg loss=1.41, train acc=0.869] \n",
      "Epoch 147 / 400: : 157it [00:04, 34.19it/s, val acc=0.817]\n",
      "Epoch 148 / 400: : 643it [01:30,  7.09it/s, lr=0.1, iterations=95807, loss=1.32, total loss=1.61, avg loss=1.41, train acc=0.866] \n",
      "Epoch 148 / 400: : 157it [00:04, 33.64it/s, val acc=0.799]\n",
      "Epoch 149 / 400: : 643it [01:30,  7.08it/s, lr=0.1, iterations=96450, loss=1.11, total loss=1.39, avg loss=1.41, train acc=0.867] \n",
      "Epoch 149 / 400: : 157it [00:04, 33.95it/s, val acc=0.759]\n",
      "Epoch 150 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=97093, loss=0.994, total loss=1.28, avg loss=1.33, train acc=0.909]\n",
      "Epoch 150 / 400: : 157it [00:04, 33.74it/s, val acc=0.911]\n",
      "Epoch 151 / 400: : 643it [01:30,  7.11it/s, lr=0.01, iterations=97736, loss=0.923, total loss=1.2, avg loss=1.27, train acc=0.935] \n",
      "Epoch 151 / 400: : 157it [00:04, 33.64it/s, val acc=0.92] \n",
      "Epoch 152 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=98379, loss=1.01, total loss=1.27, avg loss=1.25, train acc=0.943] \n",
      "Epoch 152 / 400: : 157it [00:04, 33.92it/s, val acc=0.921]\n",
      "Epoch 153 / 400: : 643it [01:30,  7.09it/s, lr=0.01, iterations=99022, loss=0.989, total loss=1.25, avg loss=1.23, train acc=0.947]\n",
      "Epoch 153 / 400: : 157it [00:04, 33.75it/s, val acc=0.924]\n",
      "Epoch 154 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=99665, loss=0.981, total loss=1.23, avg loss=1.22, train acc=0.953]\n",
      "Epoch 154 / 400: : 157it [00:04, 33.72it/s, val acc=0.924]\n",
      "Epoch 155 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=1e+5, loss=0.947, total loss=1.19, avg loss=1.2, train acc=0.955]  \n",
      "Epoch 155 / 400: : 157it [00:04, 33.57it/s, val acc=0.923]\n",
      "Epoch 156 / 400: : 643it [01:30,  7.13it/s, lr=0.01, iterations=100951, loss=0.91, total loss=1.15, avg loss=1.19, train acc=0.96]  \n",
      "Epoch 156 / 400: : 157it [00:04, 33.87it/s, val acc=0.926]\n",
      "Epoch 157 / 400: : 643it [01:30,  7.09it/s, lr=0.01, iterations=101594, loss=0.977, total loss=1.21, avg loss=1.18, train acc=0.962]\n",
      "Epoch 157 / 400: : 157it [00:04, 33.78it/s, val acc=0.922]\n",
      "Epoch 158 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=102237, loss=0.924, total loss=1.15, avg loss=1.17, train acc=0.963]\n",
      "Epoch 158 / 400: : 157it [00:04, 33.70it/s, val acc=0.927]\n",
      "Epoch 159 / 400: : 643it [01:30,  7.11it/s, lr=0.01, iterations=102880, loss=0.999, total loss=1.22, avg loss=1.16, train acc=0.965]\n",
      "Epoch 159 / 400: : 157it [00:04, 33.84it/s, val acc=0.928]\n",
      "Epoch 160 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=103523, loss=0.904, total loss=1.12, avg loss=1.16, train acc=0.967]\n",
      "Epoch 160 / 400: : 157it [00:04, 33.78it/s, val acc=0.925]\n",
      "Epoch 161 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=104166, loss=0.985, total loss=1.2, avg loss=1.15, train acc=0.968] \n",
      "Epoch 161 / 400: : 157it [00:04, 33.86it/s, val acc=0.927]\n",
      "Epoch 162 / 400: : 643it [01:30,  7.11it/s, lr=0.01, iterations=104809, loss=0.891, total loss=1.1, avg loss=1.14, train acc=0.97]  \n",
      "Epoch 162 / 400: : 157it [00:05, 31.07it/s, val acc=0.924]\n",
      "Epoch 163 / 400: : 643it [01:30,  7.13it/s, lr=0.01, iterations=105452, loss=0.988, total loss=1.19, avg loss=1.13, train acc=0.969]\n",
      "Epoch 163 / 400: : 157it [00:04, 33.84it/s, val acc=0.922]\n",
      "Epoch 164 / 400: : 643it [01:30,  7.11it/s, lr=0.01, iterations=106095, loss=0.932, total loss=1.13, avg loss=1.13, train acc=0.971]\n",
      "Epoch 164 / 400: : 157it [00:04, 33.81it/s, val acc=0.919]\n",
      "Epoch 165 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=106738, loss=0.882, total loss=1.08, avg loss=1.12, train acc=0.972]\n",
      "Epoch 165 / 400: : 157it [00:04, 33.95it/s, val acc=0.925]\n",
      "Epoch 166 / 400: : 643it [01:30,  7.09it/s, lr=0.01, iterations=107381, loss=0.912, total loss=1.1, avg loss=1.12, train acc=0.972] \n",
      "Epoch 166 / 400: : 157it [00:04, 33.85it/s, val acc=0.924]\n",
      "Epoch 167 / 400: : 643it [01:30,  7.12it/s, lr=0.01, iterations=108024, loss=0.929, total loss=1.12, avg loss=1.11, train acc=0.973]\n",
      "Epoch 167 / 400: : 157it [00:04, 33.83it/s, val acc=0.921]\n",
      "Epoch 168 / 400: : 643it [01:29,  7.14it/s, lr=0.01, iterations=108667, loss=0.937, total loss=1.12, avg loss=1.11, train acc=0.973]\n",
      "Epoch 168 / 400: : 157it [00:04, 33.67it/s, val acc=0.921]\n",
      "Epoch 169 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=109310, loss=0.939, total loss=1.12, avg loss=1.1, train acc=0.974]\n",
      "Epoch 169 / 400: : 157it [00:04, 32.92it/s, val acc=0.922]\n",
      "Epoch 170 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=109953, loss=0.943, total loss=1.12, avg loss=1.09, train acc=0.975]\n",
      "Epoch 170 / 400: : 157it [00:04, 33.54it/s, val acc=0.922]\n",
      "Epoch 171 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=110596, loss=0.908, total loss=1.08, avg loss=1.09, train acc=0.975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 400: : 157it [00:04, 33.88it/s, val acc=0.922]\n",
      "Epoch 172 / 400: : 643it [01:30,  7.07it/s, lr=0.01, iterations=111239, loss=0.919, total loss=1.09, avg loss=1.09, train acc=0.976]\n",
      "Epoch 172 / 400: : 157it [00:04, 33.74it/s, val acc=0.921]\n",
      "Epoch 173 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=111882, loss=0.9, total loss=1.07, avg loss=1.08, train acc=0.977]  \n",
      "Epoch 173 / 400: : 157it [00:04, 33.97it/s, val acc=0.923]\n",
      "Epoch 174 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=112525, loss=0.903, total loss=1.07, avg loss=1.08, train acc=0.977]\n",
      "Epoch 174 / 400: : 157it [00:04, 33.89it/s, val acc=0.926]\n",
      "Epoch 175 / 400: : 643it [01:30,  7.09it/s, lr=0.01, iterations=113168, loss=0.93, total loss=1.09, avg loss=1.08, train acc=0.975] \n",
      "Epoch 175 / 400: : 157it [00:04, 33.71it/s, val acc=0.922]\n",
      "Epoch 176 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=113811, loss=0.876, total loss=1.03, avg loss=1.07, train acc=0.975]\n",
      "Epoch 176 / 400: : 157it [00:04, 33.69it/s, val acc=0.921]\n",
      "Epoch 177 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=114454, loss=1.03, total loss=1.19, avg loss=1.07, train acc=0.975] \n",
      "Epoch 177 / 400: : 157it [00:05, 30.93it/s, val acc=0.924]\n",
      "Epoch 178 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=115097, loss=0.895, total loss=1.05, avg loss=1.07, train acc=0.976]\n",
      "Epoch 178 / 400: : 157it [00:04, 33.92it/s, val acc=0.925]\n",
      "Epoch 179 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=115740, loss=0.905, total loss=1.06, avg loss=1.07, train acc=0.976]\n",
      "Epoch 179 / 400: : 157it [00:04, 33.83it/s, val acc=0.92] \n",
      "Epoch 180 / 400: : 643it [01:30,  7.07it/s, lr=0.01, iterations=116383, loss=1.01, total loss=1.16, avg loss=1.07, train acc=0.976] \n",
      "Epoch 180 / 400: : 157it [00:04, 33.77it/s, val acc=0.916]\n",
      "Epoch 181 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=117026, loss=0.92, total loss=1.07, avg loss=1.06, train acc=0.976] \n",
      "Epoch 181 / 400: : 157it [00:04, 33.72it/s, val acc=0.918]\n",
      "Epoch 182 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=117669, loss=0.878, total loss=1.02, avg loss=1.06, train acc=0.977]\n",
      "Epoch 182 / 400: : 157it [00:04, 33.83it/s, val acc=0.924]\n",
      "Epoch 183 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=118312, loss=0.904, total loss=1.05, avg loss=1.06, train acc=0.976]\n",
      "Epoch 183 / 400: : 157it [00:04, 33.30it/s, val acc=0.915]\n",
      "Epoch 184 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=118955, loss=0.893, total loss=1.03, avg loss=1.06, train acc=0.977]\n",
      "Epoch 184 / 400: : 157it [00:04, 33.82it/s, val acc=0.918]\n",
      "Epoch 185 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=119598, loss=0.92, total loss=1.06, avg loss=1.05, train acc=0.976] \n",
      "Epoch 185 / 400: : 157it [00:04, 33.95it/s, val acc=0.92] \n",
      "Epoch 186 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=120241, loss=0.895, total loss=1.03, avg loss=1.05, train acc=0.976]\n",
      "Epoch 186 / 400: : 157it [00:04, 33.71it/s, val acc=0.92] \n",
      "Epoch 187 / 400: : 643it [01:32,  6.96it/s, lr=0.01, iterations=120884, loss=0.943, total loss=1.08, avg loss=1.05, train acc=0.976]\n",
      "Epoch 187 / 400: : 157it [00:04, 33.68it/s, val acc=0.916]\n",
      "Epoch 188 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=121527, loss=0.883, total loss=1.02, avg loss=1.05, train acc=0.975]\n",
      "Epoch 188 / 400: : 157it [00:04, 33.61it/s, val acc=0.919]\n",
      "Epoch 189 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=122170, loss=0.899, total loss=1.03, avg loss=1.05, train acc=0.976]\n",
      "Epoch 189 / 400: : 157it [00:04, 33.67it/s, val acc=0.916]\n",
      "Epoch 190 / 400: : 643it [01:31,  7.00it/s, lr=0.01, iterations=122813, loss=0.877, total loss=1.01, avg loss=1.05, train acc=0.976]\n",
      "Epoch 190 / 400: : 157it [00:05, 30.72it/s, val acc=0.917]\n",
      "Epoch 191 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=123456, loss=0.918, total loss=1.05, avg loss=1.05, train acc=0.976]\n",
      "Epoch 191 / 400: : 157it [00:04, 33.67it/s, val acc=0.921]\n",
      "Epoch 192 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=124099, loss=0.931, total loss=1.06, avg loss=1.04, train acc=0.975] \n",
      "Epoch 192 / 400: : 157it [00:04, 33.77it/s, val acc=0.921]\n",
      "Epoch 193 / 400: : 643it [01:32,  6.97it/s, lr=0.01, iterations=124742, loss=0.929, total loss=1.06, avg loss=1.04, train acc=0.975] \n",
      "Epoch 193 / 400: : 157it [00:04, 33.66it/s, val acc=0.919]\n",
      "Epoch 194 / 400: : 643it [01:32,  6.97it/s, lr=0.01, iterations=125385, loss=0.885, total loss=1.01, avg loss=1.04, train acc=0.975] \n",
      "Epoch 194 / 400: : 157it [00:04, 33.33it/s, val acc=0.915]\n",
      "Epoch 195 / 400: : 643it [01:32,  6.98it/s, lr=0.01, iterations=126028, loss=0.947, total loss=1.07, avg loss=1.04, train acc=0.975] \n",
      "Epoch 195 / 400: : 157it [00:04, 33.65it/s, val acc=0.913]\n",
      "Epoch 196 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=126671, loss=1.05, total loss=1.17, avg loss=1.04, train acc=0.976]  \n",
      "Epoch 196 / 400: : 157it [00:05, 28.35it/s, val acc=0.92] \n",
      "Epoch 197 / 400: : 643it [01:31,  6.99it/s, lr=0.01, iterations=127314, loss=1.01, total loss=1.13, avg loss=1.04, train acc=0.975]  \n",
      "Epoch 197 / 400: : 157it [00:04, 33.94it/s, val acc=0.918]\n",
      "Epoch 198 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=127957, loss=0.908, total loss=1.03, avg loss=1.04, train acc=0.975] \n",
      "Epoch 198 / 400: : 157it [00:04, 33.57it/s, val acc=0.911]\n",
      "Epoch 199 / 400: : 643it [01:32,  6.96it/s, lr=0.01, iterations=128600, loss=0.936, total loss=1.06, avg loss=1.04, train acc=0.976] \n",
      "Epoch 199 / 400: : 157it [00:04, 33.43it/s, val acc=0.913]\n",
      "Epoch 200 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=129243, loss=0.918, total loss=1.04, avg loss=1.04, train acc=0.974] \n",
      "Epoch 200 / 400: : 157it [00:04, 33.73it/s, val acc=0.918]\n",
      "Epoch 201 / 400: : 643it [01:32,  6.97it/s, lr=0.01, iterations=129886, loss=0.948, total loss=1.07, avg loss=1.03, train acc=0.977] \n",
      "Epoch 201 / 400: : 157it [00:04, 33.76it/s, val acc=0.911]\n",
      "Epoch 202 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=130529, loss=0.89, total loss=1.01, avg loss=1.03, train acc=0.974]  \n",
      "Epoch 202 / 400: : 157it [00:04, 32.04it/s, val acc=0.92] \n",
      "Epoch 203 / 400: : 643it [01:32,  6.99it/s, lr=0.01, iterations=131172, loss=0.999, total loss=1.12, avg loss=1.03, train acc=0.975] \n",
      "Epoch 203 / 400: : 157it [00:07, 20.89it/s, val acc=0.918]\n",
      "Epoch 204 / 400: : 643it [01:32,  6.92it/s, lr=0.01, iterations=131815, loss=0.957, total loss=1.07, avg loss=1.03, train acc=0.976] \n",
      "Epoch 204 / 400: : 157it [00:04, 33.47it/s, val acc=0.919]\n",
      "Epoch 205 / 400: : 643it [01:32,  6.93it/s, lr=0.01, iterations=132458, loss=0.996, total loss=1.11, avg loss=1.03, train acc=0.975] \n",
      "Epoch 205 / 400: : 157it [00:05, 30.54it/s, val acc=0.918]\n",
      "Epoch 206 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=133101, loss=0.887, total loss=1, avg loss=1.03, train acc=0.975]    \n",
      "Epoch 206 / 400: : 157it [00:04, 33.94it/s, val acc=0.903]\n",
      "Epoch 207 / 400: : 643it [01:31,  7.00it/s, lr=0.01, iterations=133744, loss=0.876, total loss=0.99, avg loss=1.03, train acc=0.974] \n",
      "Epoch 207 / 400: : 157it [00:04, 33.57it/s, val acc=0.911]\n",
      "Epoch 208 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=134387, loss=0.922, total loss=1.03, avg loss=1.03, train acc=0.975] \n",
      "Epoch 208 / 400: : 157it [00:04, 33.71it/s, val acc=0.913]\n",
      "Epoch 209 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=135030, loss=0.908, total loss=1.02, avg loss=1.03, train acc=0.974] \n",
      "Epoch 209 / 400: : 157it [00:04, 33.33it/s, val acc=0.914]\n",
      "Epoch 210 / 400: : 643it [01:32,  6.98it/s, lr=0.01, iterations=135673, loss=0.922, total loss=1.03, avg loss=1.03, train acc=0.974] \n",
      "Epoch 210 / 400: : 157it [00:04, 34.07it/s, val acc=0.905]\n",
      "Epoch 211 / 400: : 643it [01:31,  7.04it/s, lr=0.01, iterations=136316, loss=0.88, total loss=0.992, avg loss=1.03, train acc=0.974] \n",
      "Epoch 211 / 400: : 157it [00:04, 33.77it/s, val acc=0.918]\n",
      "Epoch 212 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=136959, loss=1.03, total loss=1.14, avg loss=1.03, train acc=0.974]  \n",
      "Epoch 212 / 400: : 157it [00:04, 34.02it/s, val acc=0.909]\n",
      "Epoch 213 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=137602, loss=1.01, total loss=1.12, avg loss=1.03, train acc=0.973]  \n",
      "Epoch 213 / 400: : 157it [00:04, 34.11it/s, val acc=0.786]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 400: : 643it [01:30,  7.12it/s, lr=0.01, iterations=138245, loss=0.926, total loss=1.04, avg loss=1.03, train acc=0.974] \n",
      "Epoch 214 / 400: : 157it [00:04, 33.97it/s, val acc=0.919]\n",
      "Epoch 215 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=138888, loss=0.92, total loss=1.03, avg loss=1.03, train acc=0.975]  \n",
      "Epoch 215 / 400: : 157it [00:04, 33.89it/s, val acc=0.919]\n",
      "Epoch 216 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=139531, loss=0.889, total loss=0.999, avg loss=1.03, train acc=0.975]\n",
      "Epoch 216 / 400: : 157it [00:04, 33.80it/s, val acc=0.92] \n",
      "Epoch 217 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=140174, loss=0.984, total loss=1.09, avg loss=1.03, train acc=0.976] \n",
      "Epoch 217 / 400: : 157it [00:04, 33.85it/s, val acc=0.916]\n",
      "Epoch 218 / 400: : 643it [01:30,  7.09it/s, lr=0.01, iterations=140817, loss=0.934, total loss=1.04, avg loss=1.03, train acc=0.976] \n",
      "Epoch 218 / 400: : 157it [00:04, 34.02it/s, val acc=0.91] \n",
      "Epoch 219 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=141460, loss=1.06, total loss=1.17, avg loss=1.03, train acc=0.973]  \n",
      "Epoch 219 / 400: : 157it [00:04, 33.74it/s, val acc=0.916]\n",
      "Epoch 220 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=142103, loss=0.934, total loss=1.04, avg loss=1.03, train acc=0.974] \n",
      "Epoch 220 / 400: : 157it [00:04, 33.72it/s, val acc=0.915]\n",
      "Epoch 221 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=142746, loss=0.954, total loss=1.06, avg loss=1.03, train acc=0.973] \n",
      "Epoch 221 / 400: : 157it [00:04, 33.72it/s, val acc=0.912]\n",
      "Epoch 222 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=143389, loss=0.939, total loss=1.05, avg loss=1.03, train acc=0.974] \n",
      "Epoch 222 / 400: : 157it [00:04, 34.05it/s, val acc=0.909]\n",
      "Epoch 223 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=144032, loss=0.973, total loss=1.08, avg loss=1.03, train acc=0.973] \n",
      "Epoch 223 / 400: : 157it [00:04, 33.68it/s, val acc=0.913]\n",
      "Epoch 224 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=144675, loss=0.907, total loss=1.01, avg loss=1.02, train acc=0.975] \n",
      "Epoch 224 / 400: : 157it [00:04, 33.90it/s, val acc=0.912]\n",
      "Epoch 225 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=145318, loss=0.875, total loss=0.983, avg loss=1.03, train acc=0.973]\n",
      "Epoch 225 / 400: : 157it [00:04, 33.87it/s, val acc=0.914]\n",
      "Epoch 226 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=145961, loss=0.879, total loss=0.986, avg loss=1.02, train acc=0.974]\n",
      "Epoch 226 / 400: : 157it [00:04, 32.47it/s, val acc=0.917]\n",
      "Epoch 227 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=146604, loss=0.954, total loss=1.06, avg loss=1.03, train acc=0.974] \n",
      "Epoch 227 / 400: : 157it [00:04, 33.86it/s, val acc=0.908]\n",
      "Epoch 228 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=147247, loss=0.908, total loss=1.01, avg loss=1.02, train acc=0.975] \n",
      "Epoch 228 / 400: : 157it [00:04, 33.67it/s, val acc=0.917]\n",
      "Epoch 229 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=147890, loss=0.967, total loss=1.07, avg loss=1.02, train acc=0.974] \n",
      "Epoch 229 / 400: : 157it [00:04, 33.60it/s, val acc=0.908]\n",
      "Epoch 230 / 400: : 643it [01:31,  7.03it/s, lr=0.01, iterations=148533, loss=0.905, total loss=1.01, avg loss=1.03, train acc=0.973] \n",
      "Epoch 230 / 400: : 157it [00:04, 33.88it/s, val acc=0.912]\n",
      "Epoch 231 / 400: : 643it [01:32,  6.98it/s, lr=0.01, iterations=149176, loss=0.898, total loss=1.01, avg loss=1.03, train acc=0.973] \n",
      "Epoch 231 / 400: : 157it [00:04, 33.90it/s, val acc=0.914]\n",
      "Epoch 232 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=149819, loss=1.01, total loss=1.12, avg loss=1.02, train acc=0.974]  \n",
      "Epoch 232 / 400: : 157it [00:04, 33.92it/s, val acc=0.914]\n",
      "Epoch 233 / 400: : 643it [01:31,  7.00it/s, lr=0.01, iterations=150462, loss=0.954, total loss=1.06, avg loss=1.03, train acc=0.972] \n",
      "Epoch 233 / 400: : 157it [00:04, 33.80it/s, val acc=0.906]\n",
      "Epoch 234 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=151105, loss=0.935, total loss=1.04, avg loss=1.02, train acc=0.974] \n",
      "Epoch 234 / 400: : 157it [00:04, 33.84it/s, val acc=0.92] \n",
      "Epoch 235 / 400: : 643it [01:31,  7.05it/s, lr=0.01, iterations=151748, loss=0.913, total loss=1.02, avg loss=1.02, train acc=0.975] \n",
      "Epoch 235 / 400: : 157it [00:04, 33.81it/s, val acc=0.908]\n",
      "Epoch 236 / 400: : 643it [01:32,  6.95it/s, lr=0.01, iterations=152391, loss=0.907, total loss=1.01, avg loss=1.02, train acc=0.974] \n",
      "Epoch 236 / 400: : 157it [00:04, 34.08it/s, val acc=0.908]\n",
      "Epoch 237 / 400: : 643it [01:31,  7.01it/s, lr=0.01, iterations=153034, loss=0.916, total loss=1.02, avg loss=1.02, train acc=0.974] \n",
      "Epoch 237 / 400: : 157it [00:04, 32.26it/s, val acc=0.915]\n",
      "Epoch 238 / 400: : 643it [01:32,  6.96it/s, lr=0.01, iterations=153677, loss=0.936, total loss=1.04, avg loss=1.02, train acc=0.974] \n",
      "Epoch 238 / 400: : 157it [00:04, 33.77it/s, val acc=0.917]\n",
      "Epoch 239 / 400: : 643it [01:32,  6.94it/s, lr=0.01, iterations=154320, loss=1.03, total loss=1.14, avg loss=1.02, train acc=0.974]  \n",
      "Epoch 239 / 400: : 157it [00:04, 33.87it/s, val acc=0.909]\n",
      "Epoch 240 / 400: : 643it [01:32,  6.96it/s, lr=0.01, iterations=154963, loss=0.911, total loss=1.02, avg loss=1.02, train acc=0.975] \n",
      "Epoch 240 / 400: : 157it [00:04, 33.81it/s, val acc=0.922]\n",
      "Epoch 241 / 400: : 643it [01:32,  6.99it/s, lr=0.01, iterations=155606, loss=0.881, total loss=0.987, avg loss=1.02, train acc=0.974]\n",
      "Epoch 241 / 400: : 157it [00:05, 31.28it/s, val acc=0.914]\n",
      "Epoch 242 / 400: : 643it [01:31,  7.02it/s, lr=0.01, iterations=156249, loss=0.925, total loss=1.03, avg loss=1.02, train acc=0.974] \n",
      "Epoch 242 / 400: : 157it [00:04, 33.79it/s, val acc=0.915]\n",
      "Epoch 243 / 400: : 643it [01:30,  7.08it/s, lr=0.01, iterations=156892, loss=0.877, total loss=0.983, avg loss=1.02, train acc=0.976]\n",
      "Epoch 243 / 400: : 157it [00:04, 32.63it/s, val acc=0.915]\n",
      "Epoch 244 / 400: : 643it [01:31,  7.00it/s, lr=0.01, iterations=157535, loss=0.894, total loss=0.999, avg loss=1.02, train acc=0.975]\n",
      "Epoch 244 / 400: : 157it [00:04, 32.24it/s, val acc=0.912]\n",
      "Epoch 245 / 400: : 643it [01:32,  6.96it/s, lr=0.01, iterations=158178, loss=0.961, total loss=1.07, avg loss=1.02, train acc=0.973] \n",
      "Epoch 245 / 400: : 157it [00:05, 29.28it/s, val acc=0.919]\n",
      "Epoch 246 / 400: : 643it [01:31,  6.99it/s, lr=0.01, iterations=158821, loss=0.952, total loss=1.06, avg loss=1.02, train acc=0.976] \n",
      "Epoch 246 / 400: : 157it [00:04, 33.92it/s, val acc=0.915]\n",
      "Epoch 247 / 400: : 643it [01:30,  7.12it/s, lr=0.01, iterations=159464, loss=0.907, total loss=1.01, avg loss=1.02, train acc=0.977] \n",
      "Epoch 247 / 400: : 157it [00:04, 33.66it/s, val acc=0.914]\n",
      "Epoch 248 / 400: : 643it [01:31,  7.00it/s, lr=0.01, iterations=160107, loss=0.907, total loss=1.01, avg loss=1.02, train acc=0.975] \n",
      "Epoch 248 / 400: : 157it [00:04, 33.77it/s, val acc=0.91] \n",
      "Epoch 249 / 400: : 643it [01:31,  7.06it/s, lr=0.01, iterations=160750, loss=0.883, total loss=0.989, avg loss=1.02, train acc=0.975]\n",
      "Epoch 249 / 400: : 157it [00:04, 33.84it/s, val acc=0.913]\n",
      "Epoch 250 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=161393, loss=1.04, total loss=1.15, avg loss=1.01, train acc=0.983]  \n",
      "Epoch 250 / 400: : 157it [00:04, 33.77it/s, val acc=0.932]\n",
      "Epoch 251 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=162036, loss=0.889, total loss=0.994, avg loss=0.995, train acc=0.99] \n",
      "Epoch 251 / 400: : 157it [00:04, 33.75it/s, val acc=0.932]\n",
      "Epoch 252 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=162679, loss=0.869, total loss=0.974, avg loss=0.991, train acc=0.991]\n",
      "Epoch 252 / 400: : 157it [00:04, 33.77it/s, val acc=0.934]\n",
      "Epoch 253 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=163322, loss=0.875, total loss=0.979, avg loss=0.99, train acc=0.991] \n",
      "Epoch 253 / 400: : 157it [00:04, 33.26it/s, val acc=0.935]\n",
      "Epoch 254 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=163965, loss=0.869, total loss=0.973, avg loss=0.987, train acc=0.993]\n",
      "Epoch 254 / 400: : 157it [00:04, 32.51it/s, val acc=0.935]\n",
      "Epoch 255 / 400: : 643it [01:30,  7.10it/s, lr=0.001, iterations=164608, loss=0.878, total loss=0.981, avg loss=0.985, train acc=0.993]\n",
      "Epoch 255 / 400: : 157it [00:04, 33.65it/s, val acc=0.937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 400: : 643it [01:30,  7.10it/s, lr=0.001, iterations=165251, loss=0.894, total loss=0.997, avg loss=0.984, train acc=0.994]\n",
      "Epoch 256 / 400: : 157it [00:04, 33.96it/s, val acc=0.938]\n",
      "Epoch 257 / 400: : 643it [01:30,  7.12it/s, lr=0.001, iterations=165894, loss=0.869, total loss=0.972, avg loss=0.984, train acc=0.994]\n",
      "Epoch 257 / 400: : 157it [00:04, 33.86it/s, val acc=0.938]\n",
      "Epoch 258 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=166537, loss=0.886, total loss=0.989, avg loss=0.984, train acc=0.994]\n",
      "Epoch 258 / 400: : 157it [00:04, 33.92it/s, val acc=0.937]\n",
      "Epoch 259 / 400: : 643it [01:30,  7.11it/s, lr=0.001, iterations=167180, loss=0.88, total loss=0.983, avg loss=0.983, train acc=0.993] \n",
      "Epoch 259 / 400: : 157it [00:04, 31.66it/s, val acc=0.938]\n",
      "Epoch 260 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=167823, loss=0.876, total loss=0.978, avg loss=0.981, train acc=0.995]\n",
      "Epoch 260 / 400: : 157it [00:04, 33.54it/s, val acc=0.939]\n",
      "Epoch 261 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=168466, loss=0.871, total loss=0.973, avg loss=0.981, train acc=0.994]\n",
      "Epoch 261 / 400: : 157it [00:04, 33.67it/s, val acc=0.938]\n",
      "Epoch 262 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=169109, loss=0.873, total loss=0.975, avg loss=0.98, train acc=0.995] \n",
      "Epoch 262 / 400: : 157it [00:04, 33.91it/s, val acc=0.937]\n",
      "Epoch 263 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=169752, loss=0.871, total loss=0.972, avg loss=0.98, train acc=0.995] \n",
      "Epoch 263 / 400: : 157it [00:04, 33.85it/s, val acc=0.939]\n",
      "Epoch 264 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=170395, loss=0.871, total loss=0.972, avg loss=0.978, train acc=0.996]\n",
      "Epoch 264 / 400: : 157it [00:04, 33.88it/s, val acc=0.938]\n",
      "Epoch 265 / 400: : 643it [01:30,  7.08it/s, lr=0.001, iterations=171038, loss=0.888, total loss=0.989, avg loss=0.978, train acc=0.995]\n",
      "Epoch 265 / 400: : 157it [00:04, 33.71it/s, val acc=0.939]\n",
      "Epoch 266 / 400: : 643it [01:30,  7.09it/s, lr=0.001, iterations=171681, loss=0.869, total loss=0.97, avg loss=0.977, train acc=0.995] \n",
      "Epoch 266 / 400: : 157it [00:04, 33.67it/s, val acc=0.939]\n",
      "Epoch 267 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=172324, loss=0.876, total loss=0.977, avg loss=0.978, train acc=0.995]\n",
      "Epoch 267 / 400: : 157it [00:07, 21.48it/s, val acc=0.94] \n",
      "Epoch 268 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=172967, loss=0.873, total loss=0.972, avg loss=0.977, train acc=0.996]\n",
      "Epoch 268 / 400: : 157it [00:04, 34.06it/s, val acc=0.939]\n",
      "Epoch 269 / 400: : 643it [01:31,  7.02it/s, lr=0.001, iterations=173610, loss=0.877, total loss=0.977, avg loss=0.976, train acc=0.996]\n",
      "Epoch 269 / 400: : 157it [00:04, 33.62it/s, val acc=0.937]\n",
      "Epoch 270 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=174253, loss=0.918, total loss=1.02, avg loss=0.975, train acc=0.996] \n",
      "Epoch 270 / 400: : 157it [00:04, 33.51it/s, val acc=0.938]\n",
      "Epoch 271 / 400: : 643it [01:32,  6.99it/s, lr=0.001, iterations=174896, loss=0.878, total loss=0.977, avg loss=0.975, train acc=0.996]\n",
      "Epoch 271 / 400: : 157it [00:04, 33.97it/s, val acc=0.938]\n",
      "Epoch 272 / 400: : 643it [01:32,  6.99it/s, lr=0.001, iterations=175539, loss=0.89, total loss=0.989, avg loss=0.976, train acc=0.995] \n",
      "Epoch 272 / 400: : 157it [00:06, 24.47it/s, val acc=0.94] \n",
      "Epoch 273 / 400: : 643it [01:31,  7.02it/s, lr=0.001, iterations=176182, loss=0.919, total loss=1.02, avg loss=0.974, train acc=0.996] \n",
      "Epoch 273 / 400: : 157it [00:04, 33.73it/s, val acc=0.939]\n",
      "Epoch 274 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=176825, loss=0.89, total loss=0.989, avg loss=0.974, train acc=0.996] \n",
      "Epoch 274 / 400: : 157it [00:04, 33.99it/s, val acc=0.94] \n",
      "Epoch 275 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=177468, loss=0.869, total loss=0.967, avg loss=0.973, train acc=0.997]\n",
      "Epoch 275 / 400: : 157it [00:04, 33.57it/s, val acc=0.939]\n",
      "Epoch 276 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=178111, loss=0.87, total loss=0.968, avg loss=0.973, train acc=0.996] \n",
      "Epoch 276 / 400: : 157it [00:04, 33.74it/s, val acc=0.938]\n",
      "Epoch 277 / 400: : 643it [01:32,  6.95it/s, lr=0.001, iterations=178754, loss=0.869, total loss=0.966, avg loss=0.973, train acc=0.996]\n",
      "Epoch 277 / 400: : 157it [00:04, 33.75it/s, val acc=0.939]\n",
      "Epoch 278 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=179397, loss=0.869, total loss=0.966, avg loss=0.973, train acc=0.996]\n",
      "Epoch 278 / 400: : 157it [00:04, 33.78it/s, val acc=0.94] \n",
      "Epoch 279 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=180040, loss=0.879, total loss=0.975, avg loss=0.972, train acc=0.997]\n",
      "Epoch 279 / 400: : 157it [00:04, 33.91it/s, val acc=0.94] \n",
      "Epoch 280 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=180683, loss=0.869, total loss=0.965, avg loss=0.972, train acc=0.996]\n",
      "Epoch 280 / 400: : 157it [00:04, 33.44it/s, val acc=0.939]\n",
      "Epoch 281 / 400: : 643it [01:32,  6.95it/s, lr=0.001, iterations=181326, loss=0.879, total loss=0.976, avg loss=0.972, train acc=0.996]\n",
      "Epoch 281 / 400: : 157it [00:04, 33.74it/s, val acc=0.939]\n",
      "Epoch 282 / 400: : 643it [01:32,  6.94it/s, lr=0.001, iterations=181969, loss=0.869, total loss=0.965, avg loss=0.971, train acc=0.997]\n",
      "Epoch 282 / 400: : 157it [00:04, 33.56it/s, val acc=0.941]\n",
      "Epoch 283 / 400: : 643it [01:31,  6.99it/s, lr=0.001, iterations=182612, loss=0.868, total loss=0.964, avg loss=0.97, train acc=0.997] \n",
      "Epoch 283 / 400: : 157it [00:04, 33.68it/s, val acc=0.94] \n",
      "Epoch 284 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=183255, loss=0.909, total loss=1, avg loss=0.971, train acc=0.997]    \n",
      "Epoch 284 / 400: : 157it [00:04, 32.72it/s, val acc=0.94] \n",
      "Epoch 285 / 400: : 643it [01:32,  6.94it/s, lr=0.001, iterations=183898, loss=0.9, total loss=0.996, avg loss=0.97, train acc=0.997]   \n",
      "Epoch 285 / 400: : 157it [00:05, 30.38it/s, val acc=0.94] \n",
      "Epoch 286 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=184541, loss=0.874, total loss=0.969, avg loss=0.97, train acc=0.997] \n",
      "Epoch 286 / 400: : 157it [00:04, 33.69it/s, val acc=0.941]\n",
      "Epoch 287 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=185184, loss=0.868, total loss=0.963, avg loss=0.969, train acc=0.997]\n",
      "Epoch 287 / 400: : 157it [00:04, 33.65it/s, val acc=0.94] \n",
      "Epoch 288 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=185827, loss=0.876, total loss=0.971, avg loss=0.969, train acc=0.996]\n",
      "Epoch 288 / 400: : 157it [00:04, 33.67it/s, val acc=0.941]\n",
      "Epoch 289 / 400: : 643it [01:29,  7.17it/s, lr=0.001, iterations=186470, loss=0.869, total loss=0.963, avg loss=0.969, train acc=0.996]\n",
      "Epoch 289 / 400: : 157it [00:04, 33.69it/s, val acc=0.939]\n",
      "Epoch 290 / 400: : 643it [01:31,  7.06it/s, lr=0.001, iterations=187113, loss=0.943, total loss=1.04, avg loss=0.968, train acc=0.997] \n",
      "Epoch 290 / 400: : 157it [00:04, 33.83it/s, val acc=0.941]\n",
      "Epoch 291 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=187756, loss=0.887, total loss=0.981, avg loss=0.968, train acc=0.997]\n",
      "Epoch 291 / 400: : 157it [00:04, 33.91it/s, val acc=0.941]\n",
      "Epoch 292 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=188399, loss=0.872, total loss=0.966, avg loss=0.968, train acc=0.997]\n",
      "Epoch 292 / 400: : 157it [00:04, 31.63it/s, val acc=0.941]\n",
      "Epoch 293 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=189042, loss=0.871, total loss=0.964, avg loss=0.968, train acc=0.997]\n",
      "Epoch 293 / 400: : 157it [00:04, 33.76it/s, val acc=0.941]\n",
      "Epoch 294 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=189685, loss=0.871, total loss=0.964, avg loss=0.967, train acc=0.997]\n",
      "Epoch 294 / 400: : 157it [00:04, 32.87it/s, val acc=0.94] \n",
      "Epoch 295 / 400: : 643it [01:30,  7.12it/s, lr=0.001, iterations=190328, loss=0.868, total loss=0.961, avg loss=0.967, train acc=0.997]\n",
      "Epoch 295 / 400: : 157it [00:04, 33.67it/s, val acc=0.941]\n",
      "Epoch 296 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=190971, loss=0.871, total loss=0.963, avg loss=0.967, train acc=0.997]\n",
      "Epoch 296 / 400: : 157it [00:04, 33.70it/s, val acc=0.941]\n",
      "Epoch 297 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=191614, loss=0.87, total loss=0.962, avg loss=0.967, train acc=0.997] \n",
      "Epoch 297 / 400: : 157it [00:04, 33.77it/s, val acc=0.94] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 298 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=192257, loss=0.875, total loss=0.968, avg loss=0.966, train acc=0.997]\n",
      "Epoch 298 / 400: : 157it [00:05, 30.96it/s, val acc=0.942]\n",
      "Epoch 299 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=192900, loss=0.871, total loss=0.962, avg loss=0.966, train acc=0.997]\n",
      "Epoch 299 / 400: : 157it [00:04, 33.80it/s, val acc=0.942]\n",
      "Epoch 300 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=193543, loss=0.871, total loss=0.963, avg loss=0.966, train acc=0.996]\n",
      "Epoch 300 / 400: : 157it [00:04, 33.88it/s, val acc=0.941]\n",
      "Epoch 301 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=194186, loss=0.868, total loss=0.959, avg loss=0.965, train acc=0.997]\n",
      "Epoch 301 / 400: : 157it [00:04, 33.74it/s, val acc=0.94] \n",
      "Epoch 302 / 400: : 643it [01:30,  7.10it/s, lr=0.001, iterations=194829, loss=0.871, total loss=0.962, avg loss=0.965, train acc=0.997]\n",
      "Epoch 302 / 400: : 157it [00:04, 33.71it/s, val acc=0.941]\n",
      "Epoch 303 / 400: : 643it [01:30,  7.10it/s, lr=0.001, iterations=195472, loss=0.868, total loss=0.959, avg loss=0.965, train acc=0.997]\n",
      "Epoch 303 / 400: : 157it [00:04, 33.83it/s, val acc=0.941]\n",
      "Epoch 304 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=196115, loss=0.878, total loss=0.969, avg loss=0.964, train acc=0.997]\n",
      "Epoch 304 / 400: : 157it [00:04, 33.76it/s, val acc=0.941]\n",
      "Epoch 305 / 400: : 643it [01:30,  7.08it/s, lr=0.001, iterations=196758, loss=0.87, total loss=0.96, avg loss=0.964, train acc=0.997]  \n",
      "Epoch 305 / 400: : 157it [00:04, 33.77it/s, val acc=0.941]\n",
      "Epoch 306 / 400: : 643it [01:30,  7.08it/s, lr=0.001, iterations=197401, loss=0.868, total loss=0.959, avg loss=0.963, train acc=0.997]\n",
      "Epoch 306 / 400: : 157it [00:04, 33.80it/s, val acc=0.941]\n",
      "Epoch 307 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=198044, loss=0.869, total loss=0.958, avg loss=0.964, train acc=0.997]\n",
      "Epoch 307 / 400: : 157it [00:04, 33.93it/s, val acc=0.939]\n",
      "Epoch 308 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=198687, loss=0.87, total loss=0.96, avg loss=0.963, train acc=0.997]  \n",
      "Epoch 308 / 400: : 157it [00:04, 33.76it/s, val acc=0.94] \n",
      "Epoch 309 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=199330, loss=0.885, total loss=0.975, avg loss=0.963, train acc=0.997]\n",
      "Epoch 309 / 400: : 157it [00:04, 33.61it/s, val acc=0.941]\n",
      "Epoch 310 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=2e+5, loss=0.869, total loss=0.959, avg loss=0.963, train acc=0.997]  \n",
      "Epoch 310 / 400: : 157it [00:04, 33.76it/s, val acc=0.94] \n",
      "Epoch 311 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=200616, loss=0.873, total loss=0.962, avg loss=0.963, train acc=0.997]\n",
      "Epoch 311 / 400: : 157it [00:04, 33.34it/s, val acc=0.941]\n",
      "Epoch 312 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=201259, loss=0.878, total loss=0.966, avg loss=0.962, train acc=0.997]\n",
      "Epoch 312 / 400: : 157it [00:04, 34.10it/s, val acc=0.94] \n",
      "Epoch 313 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=201902, loss=0.868, total loss=0.956, avg loss=0.962, train acc=0.997]\n",
      "Epoch 313 / 400: : 157it [00:04, 33.54it/s, val acc=0.941]\n",
      "Epoch 314 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=202545, loss=0.868, total loss=0.956, avg loss=0.962, train acc=0.997]\n",
      "Epoch 314 / 400: : 157it [00:04, 33.41it/s, val acc=0.942]\n",
      "Epoch 315 / 400: : 643it [01:32,  6.95it/s, lr=0.001, iterations=203188, loss=0.869, total loss=0.957, avg loss=0.961, train acc=0.997]\n",
      "Epoch 315 / 400: : 157it [00:04, 33.93it/s, val acc=0.942]\n",
      "Epoch 316 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=203831, loss=0.869, total loss=0.957, avg loss=0.961, train acc=0.997]\n",
      "Epoch 316 / 400: : 157it [00:04, 33.79it/s, val acc=0.941]\n",
      "Epoch 317 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=204474, loss=0.871, total loss=0.958, avg loss=0.961, train acc=0.997]\n",
      "Epoch 317 / 400: : 157it [00:04, 34.02it/s, val acc=0.941]\n",
      "Epoch 318 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=205117, loss=0.89, total loss=0.977, avg loss=0.959, train acc=0.998] \n",
      "Epoch 318 / 400: : 157it [00:04, 33.84it/s, val acc=0.941]\n",
      "Epoch 319 / 400: : 643it [01:32,  6.96it/s, lr=0.001, iterations=205760, loss=0.882, total loss=0.969, avg loss=0.96, train acc=0.997] \n",
      "Epoch 319 / 400: : 157it [00:04, 33.01it/s, val acc=0.941]\n",
      "Epoch 320 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=206403, loss=0.871, total loss=0.958, avg loss=0.959, train acc=0.998]\n",
      "Epoch 320 / 400: : 157it [00:04, 33.82it/s, val acc=0.94] \n",
      "Epoch 321 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=207046, loss=0.869, total loss=0.955, avg loss=0.96, train acc=0.997] \n",
      "Epoch 321 / 400: : 157it [00:04, 31.45it/s, val acc=0.941]\n",
      "Epoch 322 / 400: : 643it [01:32,  6.95it/s, lr=0.001, iterations=207689, loss=0.869, total loss=0.955, avg loss=0.96, train acc=0.997] \n",
      "Epoch 322 / 400: : 157it [00:04, 33.73it/s, val acc=0.941]\n",
      "Epoch 323 / 400: : 643it [01:32,  6.96it/s, lr=0.001, iterations=208332, loss=0.873, total loss=0.96, avg loss=0.96, train acc=0.997]  \n",
      "Epoch 323 / 400: : 157it [00:04, 33.59it/s, val acc=0.941]\n",
      "Epoch 324 / 400: : 643it [01:31,  6.99it/s, lr=0.001, iterations=208975, loss=0.868, total loss=0.954, avg loss=0.959, train acc=0.997]\n",
      "Epoch 324 / 400: : 157it [00:04, 33.93it/s, val acc=0.94] \n",
      "Epoch 325 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=209618, loss=0.869, total loss=0.955, avg loss=0.958, train acc=0.998]\n",
      "Epoch 325 / 400: : 157it [00:04, 34.06it/s, val acc=0.94] \n",
      "Epoch 326 / 400: : 643it [01:32,  6.97it/s, lr=0.001, iterations=210261, loss=0.879, total loss=0.964, avg loss=0.958, train acc=0.998]\n",
      "Epoch 326 / 400: : 157it [00:04, 33.58it/s, val acc=0.941]\n",
      "Epoch 327 / 400: : 643it [01:31,  6.99it/s, lr=0.001, iterations=210904, loss=0.871, total loss=0.956, avg loss=0.958, train acc=0.998]\n",
      "Epoch 327 / 400: : 157it [00:04, 33.80it/s, val acc=0.94] \n",
      "Epoch 328 / 400: : 643it [01:31,  7.02it/s, lr=0.001, iterations=211547, loss=0.868, total loss=0.953, avg loss=0.958, train acc=0.998]\n",
      "Epoch 328 / 400: : 157it [00:04, 33.79it/s, val acc=0.941]\n",
      "Epoch 329 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=212190, loss=0.868, total loss=0.953, avg loss=0.958, train acc=0.997]\n",
      "Epoch 329 / 400: : 157it [00:04, 33.76it/s, val acc=0.942]\n",
      "Epoch 330 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=212833, loss=0.869, total loss=0.954, avg loss=0.957, train acc=0.998]\n",
      "Epoch 330 / 400: : 157it [00:04, 33.58it/s, val acc=0.94] \n",
      "Epoch 331 / 400: : 643it [01:31,  7.01it/s, lr=0.001, iterations=213476, loss=0.873, total loss=0.958, avg loss=0.957, train acc=0.997]\n",
      "Epoch 331 / 400: : 157it [00:04, 33.94it/s, val acc=0.942]\n",
      "Epoch 332 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=214119, loss=0.932, total loss=1.02, avg loss=0.957, train acc=0.997] \n",
      "Epoch 332 / 400: : 157it [00:05, 31.14it/s, val acc=0.941]\n",
      "Epoch 333 / 400: : 643it [01:30,  7.08it/s, lr=0.001, iterations=214762, loss=0.875, total loss=0.959, avg loss=0.956, train acc=0.998]\n",
      "Epoch 333 / 400: : 157it [00:04, 33.64it/s, val acc=0.942]\n",
      "Epoch 334 / 400: : 643it [01:31,  7.06it/s, lr=0.001, iterations=215405, loss=0.869, total loss=0.952, avg loss=0.956, train acc=0.998]\n",
      "Epoch 334 / 400: : 157it [00:04, 33.81it/s, val acc=0.941]\n",
      "Epoch 335 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=216048, loss=0.868, total loss=0.952, avg loss=0.956, train acc=0.998]\n",
      "Epoch 335 / 400: : 157it [00:04, 33.33it/s, val acc=0.942]\n",
      "Epoch 336 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=216691, loss=0.872, total loss=0.955, avg loss=0.955, train acc=0.998]\n",
      "Epoch 336 / 400: : 157it [00:04, 31.80it/s, val acc=0.941]\n",
      "Epoch 337 / 400: : 643it [01:31,  7.00it/s, lr=0.001, iterations=217334, loss=0.873, total loss=0.956, avg loss=0.955, train acc=0.998]\n",
      "Epoch 337 / 400: : 157it [00:04, 33.58it/s, val acc=0.94] \n",
      "Epoch 338 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=217977, loss=0.868, total loss=0.951, avg loss=0.955, train acc=0.998]\n",
      "Epoch 338 / 400: : 157it [00:04, 33.61it/s, val acc=0.941]\n",
      "Epoch 339 / 400: : 643it [01:29,  7.16it/s, lr=0.001, iterations=218620, loss=0.868, total loss=0.951, avg loss=0.955, train acc=0.998]\n",
      "Epoch 339 / 400: : 157it [00:05, 30.68it/s, val acc=0.94] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 340 / 400: : 643it [01:31,  7.06it/s, lr=0.001, iterations=219263, loss=0.868, total loss=0.951, avg loss=0.955, train acc=0.998]\n",
      "Epoch 340 / 400: : 157it [00:04, 33.87it/s, val acc=0.939]\n",
      "Epoch 341 / 400: : 643it [01:30,  7.13it/s, lr=0.001, iterations=219906, loss=0.869, total loss=0.952, avg loss=0.955, train acc=0.998]\n",
      "Epoch 341 / 400: : 157it [00:04, 33.22it/s, val acc=0.941]\n",
      "Epoch 342 / 400: : 643it [01:31,  7.05it/s, lr=0.001, iterations=220549, loss=0.868, total loss=0.95, avg loss=0.955, train acc=0.998] \n",
      "Epoch 342 / 400: : 157it [00:04, 33.68it/s, val acc=0.94] \n",
      "Epoch 343 / 400: : 643it [01:31,  7.06it/s, lr=0.001, iterations=221192, loss=0.923, total loss=1, avg loss=0.955, train acc=0.997]    \n",
      "Epoch 343 / 400: : 157it [00:04, 33.89it/s, val acc=0.94] \n",
      "Epoch 344 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=221835, loss=0.868, total loss=0.949, avg loss=0.954, train acc=0.998]\n",
      "Epoch 344 / 400: : 157it [00:04, 33.95it/s, val acc=0.939]\n",
      "Epoch 345 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=222478, loss=0.869, total loss=0.95, avg loss=0.953, train acc=0.998] \n",
      "Epoch 345 / 400: : 157it [00:04, 33.82it/s, val acc=0.94] \n",
      "Epoch 346 / 400: : 643it [01:31,  7.04it/s, lr=0.001, iterations=223121, loss=0.898, total loss=0.979, avg loss=0.953, train acc=0.998]\n",
      "Epoch 346 / 400: : 157it [00:04, 33.19it/s, val acc=0.941]\n",
      "Epoch 347 / 400: : 643it [01:31,  7.03it/s, lr=0.001, iterations=223764, loss=0.884, total loss=0.965, avg loss=0.953, train acc=0.998]\n",
      "Epoch 347 / 400: : 157it [00:05, 26.98it/s, val acc=0.941]\n",
      "Epoch 348 / 400: : 643it [01:30,  7.07it/s, lr=0.001, iterations=224407, loss=0.87, total loss=0.951, avg loss=0.953, train acc=0.998] \n",
      "Epoch 348 / 400: : 157it [00:04, 33.63it/s, val acc=0.941]\n",
      "Epoch 349 / 400: : 643it [01:32,  6.98it/s, lr=0.001, iterations=225050, loss=0.867, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 349 / 400: : 157it [00:04, 33.61it/s, val acc=0.942]\n",
      "Epoch 350 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=225693, loss=0.868, total loss=0.948, avg loss=0.953, train acc=0.997]\n",
      "Epoch 350 / 400: : 157it [00:04, 33.94it/s, val acc=0.942]\n",
      "Epoch 351 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=226336, loss=0.9, total loss=0.98, avg loss=0.952, train acc=0.998]   \n",
      "Epoch 351 / 400: : 157it [00:04, 33.79it/s, val acc=0.942]\n",
      "Epoch 352 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=226979, loss=0.924, total loss=1, avg loss=0.952, train acc=0.998]    \n",
      "Epoch 352 / 400: : 157it [00:04, 33.66it/s, val acc=0.942]\n",
      "Epoch 353 / 400: : 643it [01:31,  7.02it/s, lr=1e-04, iterations=227622, loss=0.871, total loss=0.952, avg loss=0.952, train acc=0.998]\n",
      "Epoch 353 / 400: : 157it [00:04, 33.45it/s, val acc=0.941]\n",
      "Epoch 354 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=228265, loss=0.868, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 354 / 400: : 157it [00:04, 34.08it/s, val acc=0.941]\n",
      "Epoch 355 / 400: : 643it [01:31,  6.99it/s, lr=1e-04, iterations=228908, loss=0.877, total loss=0.958, avg loss=0.951, train acc=0.998]\n",
      "Epoch 355 / 400: : 157it [00:05, 27.12it/s, val acc=0.942]\n",
      "Epoch 356 / 400: : 643it [01:32,  6.93it/s, lr=1e-04, iterations=229551, loss=0.868, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 356 / 400: : 157it [00:04, 33.59it/s, val acc=0.943]\n",
      "Epoch 357 / 400: : 643it [01:32,  6.98it/s, lr=1e-04, iterations=230194, loss=0.87, total loss=0.95, avg loss=0.952, train acc=0.998]  \n",
      "Epoch 357 / 400: : 157it [00:04, 33.47it/s, val acc=0.943]\n",
      "Epoch 358 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=230837, loss=0.868, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 358 / 400: : 157it [00:04, 33.74it/s, val acc=0.943]\n",
      "Epoch 359 / 400: : 643it [01:32,  6.95it/s, lr=1e-04, iterations=231480, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 359 / 400: : 157it [00:04, 33.61it/s, val acc=0.943]\n",
      "Epoch 360 / 400: : 643it [01:31,  7.02it/s, lr=1e-04, iterations=232123, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 360 / 400: : 157it [00:04, 33.80it/s, val acc=0.943]\n",
      "Epoch 361 / 400: : 643it [01:33,  6.89it/s, lr=1e-04, iterations=232766, loss=0.877, total loss=0.957, avg loss=0.951, train acc=0.998]\n",
      "Epoch 361 / 400: : 157it [00:05, 26.68it/s, val acc=0.942]\n",
      "Epoch 362 / 400: : 643it [01:32,  6.99it/s, lr=1e-04, iterations=233409, loss=0.87, total loss=0.95, avg loss=0.951, train acc=0.998]  \n",
      "Epoch 362 / 400: : 157it [00:04, 33.49it/s, val acc=0.943]\n",
      "Epoch 363 / 400: : 643it [01:32,  6.97it/s, lr=1e-04, iterations=234052, loss=0.868, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 363 / 400: : 157it [00:04, 33.42it/s, val acc=0.943]\n",
      "Epoch 364 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=234695, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 364 / 400: : 157it [00:04, 33.77it/s, val acc=0.942]\n",
      "Epoch 365 / 400: : 643it [01:31,  7.00it/s, lr=1e-04, iterations=235338, loss=0.868, total loss=0.948, avg loss=0.951, train acc=0.999]\n",
      "Epoch 365 / 400: : 157it [00:05, 28.41it/s, val acc=0.942]\n",
      "Epoch 366 / 400: : 643it [01:32,  6.98it/s, lr=1e-04, iterations=235981, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 366 / 400: : 157it [00:04, 33.41it/s, val acc=0.943]\n",
      "Epoch 367 / 400: : 643it [01:31,  7.05it/s, lr=1e-04, iterations=236624, loss=0.869, total loss=0.949, avg loss=0.951, train acc=0.998]\n",
      "Epoch 367 / 400: : 157it [00:05, 30.61it/s, val acc=0.943]\n",
      "Epoch 368 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=237267, loss=0.867, total loss=0.948, avg loss=0.951, train acc=0.998]\n",
      "Epoch 368 / 400: : 157it [00:04, 33.72it/s, val acc=0.942]\n",
      "Epoch 369 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=237910, loss=0.868, total loss=0.948, avg loss=0.952, train acc=0.998]\n",
      "Epoch 369 / 400: : 157it [00:04, 33.82it/s, val acc=0.943]\n",
      "Epoch 370 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=238553, loss=0.872, total loss=0.952, avg loss=0.951, train acc=0.998]\n",
      "Epoch 370 / 400: : 157it [00:04, 33.67it/s, val acc=0.942]\n",
      "Epoch 371 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=239196, loss=0.914, total loss=0.994, avg loss=0.952, train acc=0.998]\n",
      "Epoch 371 / 400: : 157it [00:04, 33.83it/s, val acc=0.943]\n",
      "Epoch 372 / 400: : 643it [01:31,  7.01it/s, lr=1e-04, iterations=239839, loss=0.867, total loss=0.947, avg loss=0.952, train acc=0.998]\n",
      "Epoch 372 / 400: : 157it [00:04, 33.96it/s, val acc=0.943]\n",
      "Epoch 373 / 400: : 643it [01:30,  7.07it/s, lr=1e-04, iterations=240482, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 373 / 400: : 157it [00:04, 33.94it/s, val acc=0.943]\n",
      "Epoch 374 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=241125, loss=0.867, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 374 / 400: : 157it [00:04, 33.92it/s, val acc=0.943]\n",
      "Epoch 375 / 400: : 643it [01:31,  6.99it/s, lr=1e-04, iterations=241768, loss=0.877, total loss=0.957, avg loss=0.951, train acc=0.998]\n",
      "Epoch 375 / 400: : 157it [00:04, 33.75it/s, val acc=0.943]\n",
      "Epoch 376 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=242411, loss=0.868, total loss=0.948, avg loss=0.951, train acc=0.998]\n",
      "Epoch 376 / 400: : 157it [00:05, 31.18it/s, val acc=0.942]\n",
      "Epoch 377 / 400: : 643it [01:32,  6.93it/s, lr=1e-04, iterations=243054, loss=0.869, total loss=0.949, avg loss=0.951, train acc=0.998]\n",
      "Epoch 377 / 400: : 157it [00:04, 33.69it/s, val acc=0.942]\n",
      "Epoch 378 / 400: : 643it [01:32,  6.97it/s, lr=1e-04, iterations=243697, loss=0.869, total loss=0.949, avg loss=0.951, train acc=0.998]\n",
      "Epoch 378 / 400: : 157it [00:04, 33.52it/s, val acc=0.943]\n",
      "Epoch 379 / 400: : 643it [01:31,  7.00it/s, lr=1e-04, iterations=244340, loss=0.868, total loss=0.948, avg loss=0.951, train acc=0.998]\n",
      "Epoch 379 / 400: : 157it [00:04, 33.68it/s, val acc=0.942]\n",
      "Epoch 380 / 400: : 643it [01:31,  7.03it/s, lr=1e-04, iterations=244983, loss=0.874, total loss=0.954, avg loss=0.951, train acc=0.998]\n",
      "Epoch 380 / 400: : 157it [00:04, 32.13it/s, val acc=0.942]\n",
      "Epoch 381 / 400: : 643it [01:31,  7.04it/s, lr=1e-04, iterations=245626, loss=0.867, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 381 / 400: : 157it [00:04, 33.75it/s, val acc=0.942]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 382 / 400: : 643it [01:31,  7.00it/s, lr=1e-04, iterations=246269, loss=0.869, total loss=0.949, avg loss=0.952, train acc=0.998]\n",
      "Epoch 382 / 400: : 157it [00:04, 33.67it/s, val acc=0.943]\n",
      "Epoch 383 / 400: : 643it [01:30,  7.10it/s, lr=1e-04, iterations=246912, loss=0.868, total loss=0.948, avg loss=0.951, train acc=0.998]\n",
      "Epoch 383 / 400: : 157it [00:04, 33.45it/s, val acc=0.942]\n",
      "Epoch 384 / 400: : 643it [01:31,  7.04it/s, lr=1e-04, iterations=247555, loss=0.868, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 384 / 400: : 157it [00:04, 33.63it/s, val acc=0.942]\n",
      "Epoch 385 / 400: : 643it [01:31,  7.02it/s, lr=1e-04, iterations=248198, loss=0.888, total loss=0.968, avg loss=0.951, train acc=0.999]\n",
      "Epoch 385 / 400: : 157it [00:05, 29.32it/s, val acc=0.941]\n",
      "Epoch 386 / 400: : 643it [01:30,  7.08it/s, lr=1e-04, iterations=248841, loss=0.884, total loss=0.964, avg loss=0.951, train acc=0.998]\n",
      "Epoch 386 / 400: : 157it [00:04, 32.88it/s, val acc=0.943]\n",
      "Epoch 387 / 400: : 643it [01:33,  6.88it/s, lr=1e-04, iterations=249484, loss=0.869, total loss=0.949, avg loss=0.951, train acc=0.998]\n",
      "Epoch 387 / 400: : 157it [00:06, 23.97it/s, val acc=0.942]\n",
      "Epoch 388 / 400: : 643it [01:36,  6.68it/s, lr=1e-04, iterations=250127, loss=0.868, total loss=0.948, avg loss=0.951, train acc=0.998]\n",
      "Epoch 388 / 400: : 157it [00:10, 15.58it/s, val acc=0.942]\n",
      "Epoch 389 / 400: : 643it [01:34,  6.79it/s, lr=1e-04, iterations=250770, loss=0.867, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 389 / 400: : 157it [00:05, 26.87it/s, val acc=0.942]\n",
      "Epoch 390 / 400: : 643it [01:34,  6.84it/s, lr=1e-04, iterations=251413, loss=0.869, total loss=0.949, avg loss=0.951, train acc=0.998]\n",
      "Epoch 390 / 400: : 157it [00:06, 23.76it/s, val acc=0.942]\n",
      "Epoch 391 / 400: : 643it [01:34,  6.78it/s, lr=1e-04, iterations=252056, loss=0.868, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 391 / 400: : 157it [00:10, 15.69it/s, val acc=0.941]\n",
      "Epoch 392 / 400: : 643it [01:34,  6.81it/s, lr=1e-04, iterations=252699, loss=0.867, total loss=0.947, avg loss=0.95, train acc=0.999]\n",
      "Epoch 392 / 400: : 157it [00:04, 32.45it/s, val acc=0.942]\n",
      "Epoch 393 / 400: : 643it [01:35,  6.76it/s, lr=1e-04, iterations=253342, loss=0.876, total loss=0.956, avg loss=0.951, train acc=0.998]\n",
      "Epoch 393 / 400: : 157it [00:05, 26.69it/s, val acc=0.942]\n",
      "Epoch 394 / 400: : 643it [01:35,  6.76it/s, lr=1e-04, iterations=253985, loss=0.873, total loss=0.953, avg loss=0.951, train acc=0.998]\n",
      "Epoch 394 / 400: : 157it [00:06, 25.62it/s, val acc=0.942]\n",
      "Epoch 395 / 400: : 643it [01:34,  6.78it/s, lr=1e-04, iterations=254628, loss=0.881, total loss=0.96, avg loss=0.951, train acc=0.998] \n",
      "Epoch 395 / 400: : 157it [00:07, 22.04it/s, val acc=0.942]\n",
      "Epoch 396 / 400: : 643it [01:34,  6.78it/s, lr=1e-04, iterations=255271, loss=0.868, total loss=0.947, avg loss=0.951, train acc=0.998]\n",
      "Epoch 396 / 400: : 157it [00:08, 19.59it/s, val acc=0.942]\n",
      "Epoch 397 / 400: : 643it [01:34,  6.77it/s, lr=1e-04, iterations=255914, loss=0.874, total loss=0.953, avg loss=0.951, train acc=0.998]\n",
      "Epoch 397 / 400: : 157it [00:04, 31.98it/s, val acc=0.942]\n",
      "Epoch 398 / 400: : 643it [01:34,  6.79it/s, lr=1e-04, iterations=256557, loss=0.871, total loss=0.95, avg loss=0.951, train acc=0.998] \n",
      "Epoch 398 / 400: : 157it [00:04, 33.49it/s, val acc=0.942]\n",
      "Epoch 399 / 400: : 643it [01:34,  6.81it/s, lr=1e-04, iterations=257200, loss=0.875, total loss=0.954, avg loss=0.95, train acc=0.998] \n",
      "Epoch 399 / 400: : 157it [00:05, 28.93it/s, val acc=0.943]\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
